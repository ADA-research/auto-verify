{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Auto-Verify","text":"<p>Auto-Verify is a tool that provides interfaces, parameter spaces and installation managers for different neural network verification tools.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Warning</p> <p>Auto-Verify has only been tested for Linux and will not work on MacOS and Windows.</p>"},{"location":"#installing-auto-verify","title":"Installing Auto-Verify","text":"<p>First, install Miniconda. Miniconda is used to manage the environments of different verification tools, other environment managers will not work.</p> <p>Warning</p> <p>Anaconda can fail trying to install environments in some cases where Miniconda does not.</p> <p>After Miniconda is installed, setup Auto-Verify by running the following commands:</p> <pre><code>&gt; conda create -n auto-verify python=3.10\n&gt; conda activate auto-verify\n&gt; pip install auto-verify\n</code></pre> <p>To check if the installation was succesful, run:</p> <pre><code>&gt; auto-verify --version\n</code></pre>"},{"location":"#installing-verification-tools","title":"Installing Verification Tools","text":"<p>Currently, Auto-Verify supports the following verifiers:</p> <ul> <li>nnenum (Stanley Bak)</li> <li>AB-Crown (Zhang et al.)</li> <li>VeriNet (VAS Group)</li> <li>Oval-BaB (OVAL Research Group)</li> <li>SDP-CROWN (Chiu et al. - OVAL Research Group) \u2013 specializes in efficient robustness verification under the L2 perturbation norm using semidefinite-program-based bound propagation.</li> </ul> <p>These verifiers can be installed as follows:</p> <pre><code>&gt; auto-verify install nnenum\n&gt; auto-verify install abcrown\n&gt; auto-verify install sdpcrown\n&gt; auto-verify install verinet\n&gt; auto-verify install ovalbab\n</code></pre> <p>To uninstall a verifier, run:</p> <pre><code>&gt; auto-verify uninstall [verifier]\n</code></pre>"},{"location":"api/","title":"API","text":""},{"location":"api/#verifier","title":"Verifier","text":"<p>Base class for verifiers.</p> <p>Classes for data about verification.</p>"},{"location":"api/#autoverify.verifier.verifier.CompleteVerifier","title":"<code>CompleteVerifier</code>","text":"<p>               Bases: <code>Verifier</code></p> <p>Abstract class for complete verifiers.</p> Source code in <code>autoverify/verifier/verifier.py</code> <pre><code>class CompleteVerifier(Verifier):\n    \"\"\"Abstract class for complete verifiers.\"\"\"\n\n    def verify_property(\n        self,\n        network: Path,\n        property: Path,\n        *,\n        config: Configuration | Path | None = None,\n        timeout: int = DEFAULT_VERIFICATION_TIMEOUT_SEC,\n    ) -&gt; CompleteVerificationResult:\n        \"\"\"Verify the property on the network.\n\n        Runs the verifier and feeds the network/property instance as input.\n        Complete verification will result in one of the following four\n        possibilities: `SAT`, `UNSAT`, `TIMEOUT`, `ERR`.\n\n        Args:\n            network: The `Path` to the network in `.onnx` format.\n            property: The `Path` to the property in `.vnnlib` format.\n            config: The configuration of the verification tool to be used. If\n                `None` is passed, the default configuration of the verification\n                tool will be used.\n            timeout: The maximum number of seconds that can be spent on the\n                verification query.\n\n        Returns:\n            CompleteVerificationResult: A `Result` object containing information\n            about the verification attempt. TODO: Link docs or something\n        \"\"\"\n        self._print_verifier_path_once()\n        network, property = network.resolve(), property.resolve()\n        self._check_instance(network, property)\n\n        if config is None:\n            config = self.default_config\n\n        # Tools use different configuration formats and methods, so we let\n        # them do some initialization here\n\n        config = self._init_config(\n            network,\n            property,\n            config,\n        )\n\n        run_cmd, output_file = self._get_run_cmd(\n            network,\n            property,\n            config=config,\n            timeout=timeout,\n        )\n\n        outcome = self._run_verification(\n            run_cmd,\n            result_file=output_file,\n            timeout=timeout,\n        )\n\n        # Shutting down after timeout may take some time, so we set the took\n        # value to the actual timeout\n        if outcome.result == \"TIMEOUT\":\n            outcome.took = timeout\n\n        # TODO: What is the point of wrapping in Ok/Err here\n        return Ok(outcome) if outcome.result != \"ERR\" else Err(outcome)\n\n    def verify_instance(\n        self,\n        instance: VerificationInstance,\n        *,\n        config: Configuration | Path | None = None,\n    ) -&gt; CompleteVerificationResult:\n        \"\"\"See the `verify_property` docstring.\"\"\"\n        return self.verify_property(\n            instance.network,\n            instance.property,\n            timeout=instance.timeout,\n            config=config,\n        )\n\n    def verify_batch(\n        self,\n        instances: Iterable[VerificationInstance],\n        *,\n        config: Configuration | Path | None = None,\n    ) -&gt; list[CompleteVerificationResult]:\n        \"\"\"Verify a batch.\n\n        Not yet implemented.\n        \"\"\"\n        for instance in instances:\n            self._check_instance(instance.network, instance.property)\n\n        if config is None:\n            config = self.default_config\n\n        return self._verify_batch(\n            instances,\n            config=config,\n        )\n\n    @abstractmethod\n    def _verify_batch(\n        self,\n        instances: Iterable[VerificationInstance],\n        *,\n        config: Configuration | Path | None,\n    ) -&gt; list[CompleteVerificationResult]:\n        raise NotImplementedError\n\n    def _allocate_run_cmd(\n        self,\n        run_cmd: str,\n        contexts: list[AbstractContextManager[None]],\n    ) -&gt; str:\n        # TODO: GPU allocation\n        assert self._cpu_gpu_allocation is not None\n\n        taskset_cmd = taskset_cpu_range(self._cpu_gpu_allocation[0:2])\n        lines = []\n\n        gpu_dev = self._cpu_gpu_allocation[2]\n        gpus = nvidia_gpu_count()\n\n        if gpu_dev &gt; gpus - 1:\n            raise ValueError(f\"Asked for GPU {gpu_dev} (0-indexed), but only found {gpus} GPU(s)\")\n\n        if gpu_dev &gt;= 0:\n            contexts.append(environment(CUDA_VISIBLE_DEVICES=str(gpu_dev)))\n\n        for line in run_cmd.splitlines():\n            line = line.lstrip()\n            if len(line) == 0 or line.isspace():\n                continue\n\n            # HACK: Why does taskset not work with `source` and `conda`?\n            if line.startswith(\"source\") or line.startswith(\"conda\"):\n                lines.append(line)\n            else:\n                lines.append(taskset_cmd + \" \" + line)\n\n        return \"\\n\".join(lines)\n\n    def set_timeout_event(self):\n        \"\"\"Signal that the process has timed out.\"\"\"\n        with suppress(AttributeError):\n            self._timeout_event.set()  # type: ignore\n\n    def _run_verification(\n        self,\n        run_cmd: str,\n        *,\n        result_file: Path | None = None,\n        timeout: int = DEFAULT_VERIFICATION_TIMEOUT_SEC,\n    ) -&gt; CompleteVerificationData:\n        contexts = self.contexts or []\n        output_lines: list[str] = []\n        result: str = \"\"\n\n        if self._cpu_gpu_allocation is not None:\n            run_cmd = self._allocate_run_cmd(run_cmd, contexts)\n\n        with ExitStack() as stack:\n            for context in contexts:\n                stack.enter_context(context)\n\n            process = subprocess.Popen(\n                run_cmd,\n                executable=\"/bin/bash\",\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                shell=True,\n                universal_newlines=True,\n                preexec_fn=os.setsid,\n            )\n\n            before_t = time.time()\n            self._timeout_event: threading.Event | None = threading.Event()\n\n            def _terminate(timeout_sec):\n                assert self._timeout_event\n                on_time = self._timeout_event.wait(timeout_sec)\n\n                if not on_time:\n                    global result\n                    result = \"TIMEOUT\"  # type: ignore\n\n                if pid_exists(process.pid):\n                    os.killpg(os.getpgid(process.pid), signal.SIGTERM)\n\n            t = threading.Thread(target=_terminate, args=[timeout])\n            t.start()\n\n            assert process.stdout\n\n            for line in iter(process.stdout.readline, \"\"):\n                output_lines.append(line)\n\n            process.stdout.close()\n            return_code = process.wait()\n            took_t = time.time() - before_t\n            self._timeout_event.set()\n\n            output_str = \"\".join(output_lines)\n            counter_example: str | None = None\n\n            if result != \"TIMEOUT\":\n                if return_code &gt; 0:\n                    result = \"ERR\"\n                else:\n                    result, counter_example = self._parse_result(output_str, result_file)\n\n            self._timeout_event = None\n\n            return CompleteVerificationData(\n                result,  # type: ignore\n                took_t,\n                counter_example,\n                \"\",  # TODO: Remove err field; its piped it to stdout\n                output_str,\n            )\n\n        raise RuntimeError(\"Exception during handling of verification\")\n</code></pre>"},{"location":"api/#autoverify.verifier.verifier.CompleteVerifier.set_timeout_event","title":"<code>set_timeout_event()</code>","text":"<p>Signal that the process has timed out.</p> Source code in <code>autoverify/verifier/verifier.py</code> <pre><code>def set_timeout_event(self):\n    \"\"\"Signal that the process has timed out.\"\"\"\n    with suppress(AttributeError):\n        self._timeout_event.set()  # type: ignore\n</code></pre>"},{"location":"api/#autoverify.verifier.verifier.CompleteVerifier.verify_batch","title":"<code>verify_batch(instances, *, config=None)</code>","text":"<p>Verify a batch.</p> <p>Not yet implemented.</p> Source code in <code>autoverify/verifier/verifier.py</code> <pre><code>def verify_batch(\n    self,\n    instances: Iterable[VerificationInstance],\n    *,\n    config: Configuration | Path | None = None,\n) -&gt; list[CompleteVerificationResult]:\n    \"\"\"Verify a batch.\n\n    Not yet implemented.\n    \"\"\"\n    for instance in instances:\n        self._check_instance(instance.network, instance.property)\n\n    if config is None:\n        config = self.default_config\n\n    return self._verify_batch(\n        instances,\n        config=config,\n    )\n</code></pre>"},{"location":"api/#autoverify.verifier.verifier.CompleteVerifier.verify_instance","title":"<code>verify_instance(instance, *, config=None)</code>","text":"<p>See the <code>verify_property</code> docstring.</p> Source code in <code>autoverify/verifier/verifier.py</code> <pre><code>def verify_instance(\n    self,\n    instance: VerificationInstance,\n    *,\n    config: Configuration | Path | None = None,\n) -&gt; CompleteVerificationResult:\n    \"\"\"See the `verify_property` docstring.\"\"\"\n    return self.verify_property(\n        instance.network,\n        instance.property,\n        timeout=instance.timeout,\n        config=config,\n    )\n</code></pre>"},{"location":"api/#autoverify.verifier.verifier.CompleteVerifier.verify_property","title":"<code>verify_property(network, property, *, config=None, timeout=DEFAULT_VERIFICATION_TIMEOUT_SEC)</code>","text":"<p>Verify the property on the network.</p> <p>Runs the verifier and feeds the network/property instance as input. Complete verification will result in one of the following four possibilities: <code>SAT</code>, <code>UNSAT</code>, <code>TIMEOUT</code>, <code>ERR</code>.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Path</code> <p>The <code>Path</code> to the network in <code>.onnx</code> format.</p> required <code>property</code> <code>Path</code> <p>The <code>Path</code> to the property in <code>.vnnlib</code> format.</p> required <code>config</code> <code>Configuration | Path | None</code> <p>The configuration of the verification tool to be used. If <code>None</code> is passed, the default configuration of the verification tool will be used.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The maximum number of seconds that can be spent on the verification query.</p> <code>DEFAULT_VERIFICATION_TIMEOUT_SEC</code> <p>Returns:</p> Name Type Description <code>CompleteVerificationResult</code> <code>CompleteVerificationResult</code> <p>A <code>Result</code> object containing information</p> <code>CompleteVerificationResult</code> <p>about the verification attempt. TODO: Link docs or something</p> Source code in <code>autoverify/verifier/verifier.py</code> <pre><code>def verify_property(\n    self,\n    network: Path,\n    property: Path,\n    *,\n    config: Configuration | Path | None = None,\n    timeout: int = DEFAULT_VERIFICATION_TIMEOUT_SEC,\n) -&gt; CompleteVerificationResult:\n    \"\"\"Verify the property on the network.\n\n    Runs the verifier and feeds the network/property instance as input.\n    Complete verification will result in one of the following four\n    possibilities: `SAT`, `UNSAT`, `TIMEOUT`, `ERR`.\n\n    Args:\n        network: The `Path` to the network in `.onnx` format.\n        property: The `Path` to the property in `.vnnlib` format.\n        config: The configuration of the verification tool to be used. If\n            `None` is passed, the default configuration of the verification\n            tool will be used.\n        timeout: The maximum number of seconds that can be spent on the\n            verification query.\n\n    Returns:\n        CompleteVerificationResult: A `Result` object containing information\n        about the verification attempt. TODO: Link docs or something\n    \"\"\"\n    self._print_verifier_path_once()\n    network, property = network.resolve(), property.resolve()\n    self._check_instance(network, property)\n\n    if config is None:\n        config = self.default_config\n\n    # Tools use different configuration formats and methods, so we let\n    # them do some initialization here\n\n    config = self._init_config(\n        network,\n        property,\n        config,\n    )\n\n    run_cmd, output_file = self._get_run_cmd(\n        network,\n        property,\n        config=config,\n        timeout=timeout,\n    )\n\n    outcome = self._run_verification(\n        run_cmd,\n        result_file=output_file,\n        timeout=timeout,\n    )\n\n    # Shutting down after timeout may take some time, so we set the took\n    # value to the actual timeout\n    if outcome.result == \"TIMEOUT\":\n        outcome.took = timeout\n\n    # TODO: What is the point of wrapping in Ok/Err here\n    return Ok(outcome) if outcome.result != \"ERR\" else Err(outcome)\n</code></pre>"},{"location":"api/#autoverify.verifier.verifier.Verifier","title":"<code>Verifier</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class to represent a verifier tool.</p> Source code in <code>autoverify/verifier/verifier.py</code> <pre><code>class Verifier(ABC):\n    \"\"\"Abstract class to represent a verifier tool.\"\"\"\n\n    # TODO: GPU Mode attribute\n    def __init__(\n        self,\n        batch_size: int = 512,\n        cpu_gpu_allocation: tuple[int, int, int] | None = None,\n    ):\n        \"\"\"New instance.\n\n        This is used with super calls.\n        \"\"\"\n        self._batch_size = batch_size\n        self._cpu_gpu_allocation = cpu_gpu_allocation\n        self._printed_tool_path = False\n\n    def get_init_attributes(self) -&gt; dict[str, Any]:\n        \"\"\"Get attributes provided during initialization of the verifier.\"\"\"\n        return {\n            \"batch_size\": self._batch_size,\n        }\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Unique verifier name.\"\"\"\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def config_space(self) -&gt; ConfigurationSpace:\n        \"\"\"Configuration space to sample from.\"\"\"\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def contexts(self) -&gt; list[AbstractContextManager[None]] | None:\n        \"\"\"Contexts to run the verification in.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def tool_path(self) -&gt; Path:\n        \"\"\"The path where the verifier is installed.\"\"\"\n        tool_path = VERIFIER_DIR / self.name / TOOL_DIR_NAME\n\n        if not tool_path.exists():\n            raise FileNotFoundError(f\"Could not find installation for tool {self.name}\")\n\n        return Path(tool_path)  # mypy complains tool_path is any\n\n    @property\n    def conda_env_name(self) -&gt; str:\n        \"\"\"The conda environment name associated with the verifier.\"\"\"\n        return get_verifier_conda_env_name(self.name)\n\n    @property\n    def conda_lib_path(self) -&gt; Path:\n        return get_conda_env_lib_path(self.conda_env_name)\n\n    @property\n    def default_config(self) -&gt; Configuration:\n        \"\"\"Return the default configuration.\"\"\"\n        return self.config_space.get_default_configuration()\n\n    @abstractmethod\n    def _get_run_cmd(\n        self,\n        network: Path,\n        property: Path,\n        *,\n        config: Any,\n        timeout: int = DEFAULT_VERIFICATION_TIMEOUT_SEC,\n    ) -&gt; tuple[str, Path | None]:\n        \"\"\"Get the cli command to run the verification.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def _parse_result(\n        self,\n        output: str,\n        result_file: Path | None,\n    ) -&gt; tuple[VerificationResultString, str | None]:\n        \"\"\"Parse the output to get the result.\"\"\"\n        raise NotImplementedError\n\n    def _init_config(\n        self,\n        network: Path,\n        property: Path,\n        config: Configuration | Path,\n    ) -&gt; Any:\n        \"\"\"Init the config, return type that is needed.\"\"\"\n        return config\n\n    def _print_verifier_path_once(self):\n        \"\"\"Print the resolved verifier tool path once per instance, with bars\n        at the top and bottom.\"\"\"\n        if not self._printed_tool_path:\n            bar = \"=\" * 60\n            print(bar)\n            print(f\"[auto-verify] Using verifier '{self.name}' at: {str(self.tool_path.expanduser().resolve())}\")\n            print(bar)\n            self._printed_tool_path = True\n\n    # TODO: Overload like in ConfigSpace to distinguish between return types\n    def sample_configuration(self, *, size: int = 1) -&gt; Configuration | list[Configuration]:\n        \"\"\"Sample one or more configurations.\n\n        Args:\n            size: The number of configurations to sample.\n\n        Returns:\n            Configuration | list[Configuration]: The sampled configuration(s).\n        \"\"\"\n        return self.config_space.sample_configuration(size=size)\n\n    @staticmethod\n    def is_same_config(config1: Any, config2: Any) -&gt; bool:\n        \"\"\"Check if two configs are the same.\"\"\"\n        raise NotImplementedError\n\n    # TODO: Make this a function in util/\n    @staticmethod\n    def _check_instance(network: Path, property: Path):\n        if not check_file_extension(network, \".onnx\"):\n            raise ValueError(\"Network should be in onnx format\")\n\n        if not check_file_extension(property, \".vnnlib\"):\n            raise ValueError(\"Property should be in vnnlib format\")\n</code></pre>"},{"location":"api/#autoverify.verifier.verifier.Verifier.conda_env_name","title":"<code>conda_env_name</code>  <code>property</code>","text":"<p>The conda environment name associated with the verifier.</p>"},{"location":"api/#autoverify.verifier.verifier.Verifier.config_space","title":"<code>config_space</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Configuration space to sample from.</p>"},{"location":"api/#autoverify.verifier.verifier.Verifier.contexts","title":"<code>contexts</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Contexts to run the verification in.</p>"},{"location":"api/#autoverify.verifier.verifier.Verifier.default_config","title":"<code>default_config</code>  <code>property</code>","text":"<p>Return the default configuration.</p>"},{"location":"api/#autoverify.verifier.verifier.Verifier.name","title":"<code>name</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Unique verifier name.</p>"},{"location":"api/#autoverify.verifier.verifier.Verifier.tool_path","title":"<code>tool_path</code>  <code>property</code>","text":"<p>The path where the verifier is installed.</p>"},{"location":"api/#autoverify.verifier.verifier.Verifier.__init__","title":"<code>__init__(batch_size=512, cpu_gpu_allocation=None)</code>","text":"<p>New instance.</p> <p>This is used with super calls.</p> Source code in <code>autoverify/verifier/verifier.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 512,\n    cpu_gpu_allocation: tuple[int, int, int] | None = None,\n):\n    \"\"\"New instance.\n\n    This is used with super calls.\n    \"\"\"\n    self._batch_size = batch_size\n    self._cpu_gpu_allocation = cpu_gpu_allocation\n    self._printed_tool_path = False\n</code></pre>"},{"location":"api/#autoverify.verifier.verifier.Verifier.get_init_attributes","title":"<code>get_init_attributes()</code>","text":"<p>Get attributes provided during initialization of the verifier.</p> Source code in <code>autoverify/verifier/verifier.py</code> <pre><code>def get_init_attributes(self) -&gt; dict[str, Any]:\n    \"\"\"Get attributes provided during initialization of the verifier.\"\"\"\n    return {\n        \"batch_size\": self._batch_size,\n    }\n</code></pre>"},{"location":"api/#autoverify.verifier.verifier.Verifier.is_same_config","title":"<code>is_same_config(config1, config2)</code>  <code>staticmethod</code>","text":"<p>Check if two configs are the same.</p> Source code in <code>autoverify/verifier/verifier.py</code> <pre><code>@staticmethod\ndef is_same_config(config1: Any, config2: Any) -&gt; bool:\n    \"\"\"Check if two configs are the same.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#autoverify.verifier.verifier.Verifier.sample_configuration","title":"<code>sample_configuration(*, size=1)</code>","text":"<p>Sample one or more configurations.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The number of configurations to sample.</p> <code>1</code> <p>Returns:</p> Type Description <code>Configuration | list[Configuration]</code> <p>Configuration | list[Configuration]: The sampled configuration(s).</p> Source code in <code>autoverify/verifier/verifier.py</code> <pre><code>def sample_configuration(self, *, size: int = 1) -&gt; Configuration | list[Configuration]:\n    \"\"\"Sample one or more configurations.\n\n    Args:\n        size: The number of configurations to sample.\n\n    Returns:\n        Configuration | list[Configuration]: The sampled configuration(s).\n    \"\"\"\n    return self.config_space.sample_configuration(size=size)\n</code></pre>"},{"location":"api/#autoverify.verifier.verification_result.CompleteVerificationData","title":"<code>CompleteVerificationData</code>  <code>dataclass</code>","text":"<p>Class holding data about a verification run.</p> <p>Attributes:</p> Name Type Description <code>result</code> <code>VerificationResultString</code> <p>Outcome (e.g. SAT, UNSAT...)</p> <code>took</code> <code>float</code> <p>Walltime spent</p> <code>counter_example</code> <code>str | None</code> <p>Example that violates property (if SAT)</p> <code>err</code> <code>str</code> <p>stderr</p> <code>stdout</code> <code>str</code> <p>stdout</p> Source code in <code>autoverify/verifier/verification_result.py</code> <pre><code>@dataclass\nclass CompleteVerificationData:\n    \"\"\"Class holding data about a verification run.\n\n    Attributes:\n        result: Outcome (e.g. SAT, UNSAT...)\n        took: Walltime spent\n        counter_example: Example that violates property (if SAT)\n        err: stderr\n        stdout: stdout\n    \"\"\"\n\n    result: VerificationResultString\n    took: float\n    counter_example: str | None = None\n    obtained_labels: list[str] = None\n    err: str = \"\"\n    stdout: str = \"\"\n</code></pre>"},{"location":"api/#ab-crown","title":"AB-Crown","text":""},{"location":"api/#autoverify.verifier.complete.abcrown.AbCrown","title":"<code>AbCrown</code>","text":"<p>               Bases: <code>CompleteVerifier</code></p> <p>AB-Crown.</p> Source code in <code>autoverify/verifier/complete/abcrown/verifier.py</code> <pre><code>class AbCrown(CompleteVerifier):\n    \"\"\"AB-Crown.\"\"\"\n\n    name: str = \"abcrown\"\n    config_space: ConfigurationSpace = AbCrownConfigspace\n\n    def __init__(\n        self,\n        batch_size: int = 512,\n        cpu_gpu_allocation: tuple[int, int, int] | None = None,\n        yaml_override: dict[str, Any] | None = None,\n    ):\n        \"\"\"New instance.\"\"\"\n        if cpu_gpu_allocation and cpu_gpu_allocation[2] &lt; 0:\n            raise ValueError(\"AB-Crown CPU only mode not yet supported\")\n\n        super().__init__(batch_size, cpu_gpu_allocation)\n        self._yaml_override = yaml_override\n\n    @property\n    def contexts(self) -&gt; list[AbstractContextManager[None]]:\n        return [\n            cwd(self.tool_path / \"complete_verifier\"),\n            pkill_matches([\"python abcrown.py\"]),\n        ]\n\n    def _parse_result(\n        self,\n        output: str,\n        result_file: Path | None,\n    ) -&gt; tuple[VerificationResultString, str | None]:\n        if find_substring(\"Result: sat\", output):\n            with open(str(result_file)) as f:\n                counter_example = f.read()\n\n            return \"SAT\", counter_example\n        elif find_substring(\"Result: unsat\", output):\n            return \"UNSAT\", None\n        elif find_substring(\"Result: timeout\", output):\n            return \"TIMEOUT\", None\n\n        return \"TIMEOUT\", None\n\n    def _get_run_cmd(\n        self,\n        network: Path,\n        property: Path,\n        *,\n        config: Path,\n        timeout: int = DEFAULT_VERIFICATION_TIMEOUT_SEC,\n    ) -&gt; tuple[str, Path | None]:\n        with tmp_file(\".txt\") as tmp:\n            result_file = Path(tmp.name)\n        source_cmd = get_conda_source_cmd(get_conda_path())\n\n        run_cmd = f\"\"\"\n        {\" \".join(source_cmd)}\n        conda activate {self.conda_env_name}\n        python abcrown.py --config {str(config)} \\\n        --results_file {str(result_file)} \\\n        --timeout {str(timeout)}\n        \"\"\"\n\n        return run_cmd, result_file\n\n    def _verify_batch(\n        self,\n        instances: Iterable[Any],\n        *,\n        config: Configuration | Path | None,\n    ) -&gt; list[CompleteVerificationResult]:\n        raise NotImplementedError(\"Batch verification not supported yet\")\n\n    def _init_config(\n        self,\n        network: Path,\n        property: Path,\n        config: Configuration | Path,\n    ) -&gt; Path:\n        if isinstance(config, Configuration):\n            yaml_config = AbcrownYamlConfig.from_config(\n                config,\n                network,\n                property,\n                batch_size=self._batch_size,\n                yaml_override=self._yaml_override,\n            )\n        else:\n            yaml_config = AbcrownYamlConfig.from_yaml(\n                config,\n                network,\n                property,\n                batch_size=self._batch_size,\n                yaml_override=self._yaml_override,\n            )\n\n        return Path(yaml_config.get_yaml_file_path())\n</code></pre>"},{"location":"api/#autoverify.verifier.complete.abcrown.AbCrown.__init__","title":"<code>__init__(batch_size=512, cpu_gpu_allocation=None, yaml_override=None)</code>","text":"<p>New instance.</p> Source code in <code>autoverify/verifier/complete/abcrown/verifier.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 512,\n    cpu_gpu_allocation: tuple[int, int, int] | None = None,\n    yaml_override: dict[str, Any] | None = None,\n):\n    \"\"\"New instance.\"\"\"\n    if cpu_gpu_allocation and cpu_gpu_allocation[2] &lt; 0:\n        raise ValueError(\"AB-Crown CPU only mode not yet supported\")\n\n    super().__init__(batch_size, cpu_gpu_allocation)\n    self._yaml_override = yaml_override\n</code></pre>"},{"location":"api/#nnenum","title":"nnenum","text":""},{"location":"api/#autoverify.verifier.complete.nnenum.Nnenum","title":"<code>Nnenum</code>","text":"<p>               Bases: <code>CompleteVerifier</code></p> <p>Nnenum.</p> Source code in <code>autoverify/verifier/complete/nnenum/verifier.py</code> <pre><code>class Nnenum(CompleteVerifier):\n    \"\"\"Nnenum.\"\"\"\n\n    name: str = \"nnenum\"\n    config_space: ConfigurationSpace = NnenumConfigspace\n\n    # HACK: Should not need to instantiate a whole new instance just to\n    # change `_use_auto_settings`.\n    def __init__(\n        self,\n        batch_size: int = 512,\n        cpu_gpu_allocation: tuple[int, int, int] | None = None,\n        use_auto_settings: bool = True,\n    ):\n        \"\"\"New instance.\"\"\"\n        if cpu_gpu_allocation and cpu_gpu_allocation[2] &gt;= 0:\n            raise ValueError(\"Nnenum does not use a GPU, please set it to -1.\")\n\n        super().__init__(batch_size, cpu_gpu_allocation)\n        self._use_auto_settings = use_auto_settings\n\n    @property\n    def contexts(self) -&gt; list[AbstractContextManager[None]]:\n        return [\n            cwd(self.tool_path / \"src\"),\n            environment(OPENBLAS_NUM_THREADS=\"1\", OMP_NUM_THREADS=\"1\"),\n            pkill_matches([\"python -m nnenum.nnenum\"]),\n        ]\n\n    def _parse_result(self, _: str, result_file: Path | None) -&gt; tuple[VerificationResultString, str | None]:\n        with open(str(result_file)) as f:\n            result_txt = f.read()\n\n        first_line = result_txt.split(\"\\n\", maxsplit=1)[0]\n\n        if first_line == \"unsat\":\n            return \"UNSAT\", None\n        elif first_line == \"sat\":\n            counter_example = self._parse_counter_example(result_txt)\n            return \"SAT\", counter_example\n        elif first_line == \"timeout\":\n            return \"TIMEOUT\", None\n\n        return \"TIMEOUT\", None\n\n    def _parse_counter_example(self, result_txt: str) -&gt; str:\n        return result_txt.split(\"\\n\", maxsplit=1)[1]\n\n    def _get_run_cmd(\n        self,\n        network: Path,\n        property: Path,\n        *,\n        config: dict[str, Any],\n        timeout: int = DEFAULT_VERIFICATION_TIMEOUT_SEC,\n    ) -&gt; tuple[str, Path | None]:\n        with tmp_file(\".txt\") as tmp:\n            result_file = Path(tmp.name)\n        source_cmd = get_conda_source_cmd(get_conda_path())\n\n        # In nnenum, settings are normally passed as a one word string\n        # over the CLI. This word then selects from some pre-defined settings.\n        # We want some more control however, so we also make an option to pass\n        # a stringified dict of exact settings.\n        # The \"none\" value for settings_str is used as a flag that makes\n        # nnenum use the dict of exact settings instead.\n        settings_str = \"none\"\n        if self._use_auto_settings:\n            settings_str = \"auto\"\n            config = {}\n\n        run_cmd = f\"\"\"\n        {\" \".join(source_cmd)}\n        conda activate {self.conda_env_name}\n        python -m nnenum.nnenum {str(network)} {str(property)} {str(timeout)} \\\n        {str(result_file)} \\\n        {str(cpu_count())} \\\n        {settings_str} \\\n        {shlex.quote(str(config))} \\\n        \"\"\"\n\n        return run_cmd, result_file\n\n    def _verify_batch(\n        self,\n        instances: Iterable[Any],\n        *,\n        config: Configuration | Path | None,\n    ) -&gt; list[CompleteVerificationResult]:\n        raise NotImplementedError(\"Batch verification not supported yet\")\n\n    def _init_config(\n        self,\n        network: Path,\n        property: Path,\n        config: Configuration | Path,\n    ) -&gt; dict[str, Any]:\n        if isinstance(config, Path):\n            raise ValueError(\"Configuration files for nnenum not supported yet\")\n\n        import copy\n\n        dict_config = copy.deepcopy(dict(config))\n\n        # HACK: Cant safely evaluate `np.inf`, instead we pass it as a string\n        # that is converted back to `np.inf` in the nnenum code.\n        if dict_config[\"INF_OVERAPPROX_MIN_GEN_LIMIT\"] is True:\n            dict_config[\"OVERAPPROX_MIN_GEN_LIMIT\"] = \"_inf\"\n\n        if dict_config[\"INF_OVERAPPROX_LP_TIMEOUT\"] is True:\n            dict_config[\"OVERAPPROX_LP_TIMEOUT\"] = \"_inf\"\n\n        del dict_config[\"INF_OVERAPPROX_LP_TIMEOUT\"]\n        del dict_config[\"INF_OVERAPPROX_MIN_GEN_LIMIT\"]\n\n        return dict_config\n\n    @staticmethod\n    def is_same_config(config1: Configuration | str, config2: Configuration | str) -&gt; bool:\n        \"\"\"Return if two configs are equivalent.\"\"\"\n        if isinstance(config1, Configuration):\n            config1 = str(config1[\"settings_mode\"])  # type: ignore\n        if isinstance(config2, Configuration):\n            config2 = str(config2[\"settings_mode\"])  # type: ignore\n\n        return config1 == config2\n</code></pre>"},{"location":"api/#autoverify.verifier.complete.nnenum.Nnenum.__init__","title":"<code>__init__(batch_size=512, cpu_gpu_allocation=None, use_auto_settings=True)</code>","text":"<p>New instance.</p> Source code in <code>autoverify/verifier/complete/nnenum/verifier.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 512,\n    cpu_gpu_allocation: tuple[int, int, int] | None = None,\n    use_auto_settings: bool = True,\n):\n    \"\"\"New instance.\"\"\"\n    if cpu_gpu_allocation and cpu_gpu_allocation[2] &gt;= 0:\n        raise ValueError(\"Nnenum does not use a GPU, please set it to -1.\")\n\n    super().__init__(batch_size, cpu_gpu_allocation)\n    self._use_auto_settings = use_auto_settings\n</code></pre>"},{"location":"api/#autoverify.verifier.complete.nnenum.Nnenum.is_same_config","title":"<code>is_same_config(config1, config2)</code>  <code>staticmethod</code>","text":"<p>Return if two configs are equivalent.</p> Source code in <code>autoverify/verifier/complete/nnenum/verifier.py</code> <pre><code>@staticmethod\ndef is_same_config(config1: Configuration | str, config2: Configuration | str) -&gt; bool:\n    \"\"\"Return if two configs are equivalent.\"\"\"\n    if isinstance(config1, Configuration):\n        config1 = str(config1[\"settings_mode\"])  # type: ignore\n    if isinstance(config2, Configuration):\n        config2 = str(config2[\"settings_mode\"])  # type: ignore\n\n    return config1 == config2\n</code></pre>"},{"location":"api/#oval-bab","title":"Oval-BaB","text":""},{"location":"api/#autoverify.verifier.complete.ovalbab.OvalBab","title":"<code>OvalBab</code>","text":"<p>               Bases: <code>CompleteVerifier</code></p> <p>Oval-BaB.</p> Source code in <code>autoverify/verifier/complete/ovalbab/verifier.py</code> <pre><code>class OvalBab(CompleteVerifier):\n    \"\"\"Oval-BaB.\"\"\"\n\n    name: str = \"ovalbab\"\n    config_space: ConfigurationSpace = OvalBabConfigspace\n\n    def __init__(\n        self,\n        batch_size: int = 512,\n        cpu_gpu_allocation: tuple[int, int, int] | None = None,\n    ):\n        \"\"\"New instance.\"\"\"\n        if cpu_gpu_allocation and cpu_gpu_allocation[2] &lt; 0:\n            raise ValueError(\"Oval-BaB CPU only mode not yet supported\")\n        super().__init__(batch_size, cpu_gpu_allocation)\n\n    @property\n    def contexts(self) -&gt; list[AbstractContextManager[None]]:\n        return [\n            cwd(self.tool_path),\n            environment(LD_LIBRARY_PATH=str(find_conda_lib(self.conda_env_name, \"libcudart.so.11.0\"))),\n            pkill_matches([\"python tools/bab_tools/bab_from_vnnlib.py\"]),\n        ]\n\n    def _parse_result(\n        self,\n        _: str,\n        result_file: Path | None,\n    ) -&gt; tuple[VerificationResultString, str | None]:\n        with open(str(result_file)) as f:\n            result_text = f.read()\n\n        if find_substring(\"violated\", result_text):\n            return \"SAT\", None\n        elif find_substring(\"holds\", result_text):\n            return \"UNSAT\", None\n        return \"ERR\", None\n\n    def _get_run_cmd(\n        self,\n        network: Path,\n        property: Path,\n        *,\n        config: Path,\n        timeout: int = DEFAULT_VERIFICATION_TIMEOUT_SEC,\n    ) -&gt; tuple[str, Path | None]:\n        with tmp_file(\".txt\") as tmp:\n            result_file = Path(tmp.name)\n        source_cmd = get_conda_source_cmd(get_conda_path())\n\n        run_cmd = f\"\"\"\n        {\" \".join(source_cmd)}\n        conda activate {self.conda_env_name}\n        python tools/bab_tools/bab_from_vnnlib.py --mode run_instance \\\n        --onnx {str(network)} \\\n        --vnnlib {str(property)} \\\n        --result_file {str(result_file)} \\\n        --json {str(config)} \\\n        --instance_timeout {timeout}\n        \"\"\"\n\n        return run_cmd, result_file\n\n    def _init_config(\n        self,\n        network: Path,\n        property: Path,\n        config: Configuration | Path,\n    ) -&gt; Path:\n        if isinstance(config, Configuration):\n            json_config = OvalbabJsonConfig.from_config(config)\n        else:\n            json_config = OvalbabJsonConfig.from_json(config)\n\n        return Path(json_config.get_json_file_path())\n\n    def _verify_batch(\n        self,\n        instances: Iterable[Any],\n        *,\n        config: Configuration | Path | None,\n    ) -&gt; list[CompleteVerificationResult]:\n        raise NotImplementedError(\"Batch verification not supported yet\")\n</code></pre>"},{"location":"api/#autoverify.verifier.complete.ovalbab.OvalBab.__init__","title":"<code>__init__(batch_size=512, cpu_gpu_allocation=None)</code>","text":"<p>New instance.</p> Source code in <code>autoverify/verifier/complete/ovalbab/verifier.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 512,\n    cpu_gpu_allocation: tuple[int, int, int] | None = None,\n):\n    \"\"\"New instance.\"\"\"\n    if cpu_gpu_allocation and cpu_gpu_allocation[2] &lt; 0:\n        raise ValueError(\"Oval-BaB CPU only mode not yet supported\")\n    super().__init__(batch_size, cpu_gpu_allocation)\n</code></pre>"},{"location":"api/#verinet","title":"VeriNet","text":""},{"location":"api/#autoverify.verifier.complete.verinet.Verinet","title":"<code>Verinet</code>","text":"<p>               Bases: <code>CompleteVerifier</code></p> <p>VeriNet.</p> Source code in <code>autoverify/verifier/complete/verinet/verifier.py</code> <pre><code>class Verinet(CompleteVerifier):\n    \"\"\"VeriNet.\"\"\"\n\n    name: str = \"verinet\"\n    config_space: ConfigurationSpace = VerinetConfigspace\n\n    # HACK: Quick hack to add some attributes to a VeriNet instance.\n    # Ideally, these could be passed when calling `verify_property/instance`\n    # or inside the Configuration.\n    def __init__(\n        self,\n        batch_size: int = 512,\n        cpu_gpu_allocation: tuple[int, int, int] | None = None,\n        gpu_mode: bool = True,\n        input_shape: list[int] | None = None,\n        dnnv_simplify: bool = False,\n        transpose_matmul_weights: bool = False,\n    ):\n        \"\"\"New instance.\"\"\"\n        if cpu_gpu_allocation and cpu_gpu_allocation[2] &lt; 0:\n            raise ValueError(\"VeriNet CPU only mode not yet supported\")\n\n        super().__init__(batch_size, cpu_gpu_allocation)\n        self._gpu_mode = gpu_mode\n        self._input_shape = input_shape\n        self._dnnv_simplify = dnnv_simplify\n        self._transpose_matmul_weights = transpose_matmul_weights\n\n    @property\n    def contexts(self) -&gt; list[AbstractContextManager[None]]:\n        return [\n            cwd(self.tool_path),\n            environment(\n                OPENBLAS_NUM_THREADS=\"1\",\n                OMP_NUM_THREADS=\"1\",\n                LD_LIBRARY_PATH=str(find_conda_lib(self.conda_env_name, \"libcudart.so.11.0\")),\n            ),\n            pkill_matches(\n                [\n                    \"multiprocessing.spawn\",\n                    \"multiprocessing.forkserver\",\n                    \"python cli.py\",  # TODO: Too broad\n                ]\n            ),\n        ]\n\n    def _parse_result(\n        self,\n        output: str,\n        _: Path | None,\n    ) -&gt; tuple[VerificationResultString, str | None]:\n        if find_substring(\"STATUS:  Status.Safe\", output):\n            return \"UNSAT\", None\n        elif find_substring(\"STATUS:  Status.Unsafe\", output):\n            return \"SAT\", None\n        elif find_substring(\"STATUS:  Status.Undecided\", output):\n            return \"TIMEOUT\", None\n\n        return \"TIMEOUT\", None\n\n    def _get_run_cmd(\n        self,\n        network: Path,\n        property: Path,\n        *,\n        config: Configuration,\n        timeout: int = DEFAULT_VERIFICATION_TIMEOUT_SEC,\n    ) -&gt; tuple[str, Path | None]:\n        source_cmd = get_conda_source_cmd(get_conda_path())\n        input_shape = self._input_shape or get_input_shape(network)\n\n        # params in order:\n        # network, prop, timeout, config, input_shape, max_procs, gpu_mode,\n        # dnnv_simplify\n        run_cmd = f\"\"\"\n        {\" \".join(source_cmd)}\n        conda activate {self.conda_env_name}\n        python cli.py {str(network)} {str(property)} {timeout} \\\n        {shlex.quote(str(dict(config)))} \\\n        {shlex.quote(str(input_shape))} \\\n        {-1} \\\n        {self._gpu_mode} \\\n        {self._dnnv_simplify} \\\n        {self._transpose_matmul_weights} \\\n        \"\"\"\n\n        return run_cmd, None\n\n    def _verify_batch(\n        self,\n        instances: Iterable[Any],\n        *,\n        config: Configuration | Path | None,\n    ) -&gt; list[CompleteVerificationResult]:\n        raise NotImplementedError(\"Batch verification not supported yet\")\n</code></pre>"},{"location":"api/#autoverify.verifier.complete.verinet.Verinet.__init__","title":"<code>__init__(batch_size=512, cpu_gpu_allocation=None, gpu_mode=True, input_shape=None, dnnv_simplify=False, transpose_matmul_weights=False)</code>","text":"<p>New instance.</p> Source code in <code>autoverify/verifier/complete/verinet/verifier.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 512,\n    cpu_gpu_allocation: tuple[int, int, int] | None = None,\n    gpu_mode: bool = True,\n    input_shape: list[int] | None = None,\n    dnnv_simplify: bool = False,\n    transpose_matmul_weights: bool = False,\n):\n    \"\"\"New instance.\"\"\"\n    if cpu_gpu_allocation and cpu_gpu_allocation[2] &lt; 0:\n        raise ValueError(\"VeriNet CPU only mode not yet supported\")\n\n    super().__init__(batch_size, cpu_gpu_allocation)\n    self._gpu_mode = gpu_mode\n    self._input_shape = input_shape\n    self._dnnv_simplify = dnnv_simplify\n    self._transpose_matmul_weights = transpose_matmul_weights\n</code></pre>"},{"location":"api/#portfolio","title":"Portfolio","text":"<p>Parallel portfolio.</p> <p>Class to run parallel portfolio.</p>"},{"location":"api/#autoverify.portfolio.portfolio.ConfiguredVerifier","title":"<code>ConfiguredVerifier</code>  <code>dataclass</code>","text":"<p>Class representing an interal configured verifier.</p> <p>Attributes:</p> Name Type Description <code>verifier</code> <code>str</code> <p>Name of the verifier.</p> <code>configuration</code> <code>Configuration</code> <p>Its configuration.</p> <code>resources</code> <code>tuple[int, int] | None</code> <p>Number of cores and GPUs.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>@dataclass(frozen=True, eq=True, repr=True)\nclass ConfiguredVerifier:\n    \"\"\"Class representing an interal configured verifier.\n\n    Attributes:\n        verifier: Name of the verifier.\n        configuration: Its configuration.\n        resources: Number of cores and GPUs.\n    \"\"\"\n\n    verifier: str\n    configuration: Configuration\n    resources: tuple[int, int] | None = None\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio","title":"<code>Portfolio</code>","text":"<p>               Bases: <code>MutableSet[ConfiguredVerifier]</code></p> <p>Portfolio of ConfiguredVerifiers.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>class Portfolio(MutableSet[ConfiguredVerifier]):\n    \"\"\"Portfolio of ConfiguredVerifiers.\"\"\"\n\n    def __init__(self, *cvs: ConfiguredVerifier):\n        \"\"\"Initialize a new PF with the passed verifiers.\"\"\"\n        self._pf_set: set[ConfiguredVerifier] = set(cvs)\n        self._costs: dict[str, float] = {}\n\n    def __contains__(self, cv: object):\n        \"\"\"Check if a CV is in the PF.\"\"\"\n        # cant type annotate the func arg or mypy gets mad\n        assert isinstance(cv, ConfiguredVerifier)\n        return cv in self._pf_set\n\n    def __iter__(self):\n        \"\"\"Iterate the contents of the PF.\"\"\"\n        return iter(self._pf_set)\n\n    def __len__(self):\n        \"\"\"Number of CVs in the PF.\"\"\"\n        return len(self._pf_set)\n\n    def __str__(self):\n        \"\"\"String representation of the PF.\"\"\"\n        res = \"\"\n\n        for cv in self:\n            res += str(cv) + \"\\n\"\n\n        return res\n\n    def get_set(self):\n        \"\"\"Get the underlying set.\"\"\"\n        return self._pf_set\n\n    @property\n    def configs(self) -&gt; list[Configuration]:\n        \"\"\"All the configurations in the PF.\"\"\"\n        configs = []\n\n        for cv in self._pf_set:\n            configs.append(cv.configuration)\n\n        return configs\n\n    def get_cost(self, instance: str):\n        \"\"\"Get the currently known costs of an instance.\"\"\"\n        return self._costs[instance]\n\n    def get_costs(self, instances: Iterable[str]) -&gt; dict[str, float]:\n        \"\"\"Get costs of more than one instance.\"\"\"\n        costs: dict[str, float] = {}\n\n        for inst in instances:\n            if inst in self._costs:\n                costs[inst] = self._costs[inst]\n\n        return costs\n\n    def get_all_costs(self) -&gt; dict[str, float]:\n        \"\"\"All the recorded costs.\"\"\"\n        return self._costs\n\n    def get_mean_cost(self) -&gt; float:\n        \"\"\"Get the mean cost.\"\"\"\n        return float(np.mean(list(self._costs.values())))\n\n    def get_median_cost(self) -&gt; float:\n        \"\"\"Get the median cost.\"\"\"\n        return float(np.median(list(self._costs.values())))\n\n    def get_total_cost(self) -&gt; float:\n        \"\"\"Get the total cost.\"\"\"\n        return float(np.sum(list(self._costs.values())))\n\n    def update_costs(self, costs: Mapping[str, float]):\n        \"\"\"Upate the costs based on the new costs mapping.\"\"\"\n        for instance, cost in costs.items():\n            if instance not in self._costs:\n                self._costs[instance] = cost\n                continue\n\n            self._costs[instance] = min(self._costs[instance], cost)\n\n    def add(self, cv: ConfiguredVerifier):\n        \"\"\"Add a CV to the PF, no duplicates allowed.\"\"\"\n        if cv in self._pf_set:\n            raise ValueError(f\"{cv} is already in the portfolio\")\n\n        self._pf_set.add(cv)\n\n    def discard(self, cv: ConfiguredVerifier):\n        \"\"\"Remove a CV from the PF.\"\"\"\n        if cv not in self._pf_set:\n            raise ValueError(f\"{cv} is not in the portfolio\")\n\n        self._pf_set.discard(cv)\n\n    def reallocate_resources(self, strategy: ResourceStrategy = ResourceStrategy.Auto):\n        \"\"\"Realloacte based on current contents and given strategy.\"\"\"\n        if strategy != ResourceStrategy.Auto:\n            raise NotImplementedError(\"Given `ResourceStrategy` is not supported yet.\")\n\n        # NOTE: Should put this alloc stuff in a function\n        n_cores = cpu_count()\n        cores_per = n_cores // len(self)\n        cores_remainder = n_cores % len(self)\n\n        for cv in self:\n            verifier = cv.verifier\n            resources = cv.resources\n            config = cv.configuration\n\n            extra_core = 0\n            if cores_remainder &gt; 0:\n                extra_core = 1\n                cores_remainder -= 1\n\n            new_resources = (cores_per + extra_core, resources[1]) if resources else None\n\n            self.discard(cv)\n            self.add(ConfiguredVerifier(verifier, config, new_resources))\n\n    def to_json(self, out_json_path: Path):\n        \"\"\"Write the portfolio in JSON format to the specified path.\"\"\"\n        json_list: list[dict[str, Any]] = []\n\n        for cv in self._pf_set:\n            cfg_dict = dict(cv.configuration)\n            to_write = {\n                \"verifier\": cv.verifier,\n                \"configuration\": cfg_dict,\n                \"resources\": cv.resources,\n            }\n            json_list.append(to_write)\n\n        with open(out_json_path, \"w\") as f:\n            json.dump(json_list, f, indent=4, default=str)\n\n    @classmethod\n    def from_json(\n        cls,\n        json_file: Path,\n        config_space_map: Mapping[str, ConfigurationSpace] | None = None,\n    ) -&gt; Portfolio:\n        \"\"\"Instantiate a new Portfolio class from a JSON file.\"\"\"\n        with open(json_file.expanduser().resolve()) as f:\n            pf_json = json.load(f)\n\n        pf = Portfolio()\n\n        for cv in pf_json:\n            if config_space_map is None:\n                cfg_space = get_verifier_configspace(cv[\"verifier\"])\n            else:\n                cfg_space = config_space_map[cv[\"verifier\"]]\n\n            cv[\"configuration\"] = Configuration(cfg_space, cv[\"configuration\"])\n\n            if cv[\"resources\"]:\n                cv[\"resources\"] = tuple(cv[\"resources\"])\n\n            pf.add(ConfiguredVerifier(**cv))\n\n        return pf\n\n    def str_compact(self) -&gt; str:\n        \"\"\"Get the portfolio in string form in a compact way.\"\"\"\n        cvs: list[str] = []\n\n        for cv in self:\n            cvs.append(\n                \"\\t\".join(\n                    [\n                        str(cv.verifier),\n                        str(hash(cv.configuration)),\n                        str(cv.resources),\n                    ]\n                )\n            )\n\n        return \"\\n\".join(cvs)\n\n    def dump_costs(self):\n        \"\"\"Print the costs for each instance.\"\"\"\n        for instance, cost in self._costs.items():\n            print(instance, cost)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.configs","title":"<code>configs</code>  <code>property</code>","text":"<p>All the configurations in the PF.</p>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.__contains__","title":"<code>__contains__(cv)</code>","text":"<p>Check if a CV is in the PF.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def __contains__(self, cv: object):\n    \"\"\"Check if a CV is in the PF.\"\"\"\n    # cant type annotate the func arg or mypy gets mad\n    assert isinstance(cv, ConfiguredVerifier)\n    return cv in self._pf_set\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.__init__","title":"<code>__init__(*cvs)</code>","text":"<p>Initialize a new PF with the passed verifiers.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def __init__(self, *cvs: ConfiguredVerifier):\n    \"\"\"Initialize a new PF with the passed verifiers.\"\"\"\n    self._pf_set: set[ConfiguredVerifier] = set(cvs)\n    self._costs: dict[str, float] = {}\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate the contents of the PF.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate the contents of the PF.\"\"\"\n    return iter(self._pf_set)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.__len__","title":"<code>__len__()</code>","text":"<p>Number of CVs in the PF.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def __len__(self):\n    \"\"\"Number of CVs in the PF.\"\"\"\n    return len(self._pf_set)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.__str__","title":"<code>__str__()</code>","text":"<p>String representation of the PF.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def __str__(self):\n    \"\"\"String representation of the PF.\"\"\"\n    res = \"\"\n\n    for cv in self:\n        res += str(cv) + \"\\n\"\n\n    return res\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.add","title":"<code>add(cv)</code>","text":"<p>Add a CV to the PF, no duplicates allowed.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def add(self, cv: ConfiguredVerifier):\n    \"\"\"Add a CV to the PF, no duplicates allowed.\"\"\"\n    if cv in self._pf_set:\n        raise ValueError(f\"{cv} is already in the portfolio\")\n\n    self._pf_set.add(cv)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.discard","title":"<code>discard(cv)</code>","text":"<p>Remove a CV from the PF.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def discard(self, cv: ConfiguredVerifier):\n    \"\"\"Remove a CV from the PF.\"\"\"\n    if cv not in self._pf_set:\n        raise ValueError(f\"{cv} is not in the portfolio\")\n\n    self._pf_set.discard(cv)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.dump_costs","title":"<code>dump_costs()</code>","text":"<p>Print the costs for each instance.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def dump_costs(self):\n    \"\"\"Print the costs for each instance.\"\"\"\n    for instance, cost in self._costs.items():\n        print(instance, cost)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.from_json","title":"<code>from_json(json_file, config_space_map=None)</code>  <code>classmethod</code>","text":"<p>Instantiate a new Portfolio class from a JSON file.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    json_file: Path,\n    config_space_map: Mapping[str, ConfigurationSpace] | None = None,\n) -&gt; Portfolio:\n    \"\"\"Instantiate a new Portfolio class from a JSON file.\"\"\"\n    with open(json_file.expanduser().resolve()) as f:\n        pf_json = json.load(f)\n\n    pf = Portfolio()\n\n    for cv in pf_json:\n        if config_space_map is None:\n            cfg_space = get_verifier_configspace(cv[\"verifier\"])\n        else:\n            cfg_space = config_space_map[cv[\"verifier\"]]\n\n        cv[\"configuration\"] = Configuration(cfg_space, cv[\"configuration\"])\n\n        if cv[\"resources\"]:\n            cv[\"resources\"] = tuple(cv[\"resources\"])\n\n        pf.add(ConfiguredVerifier(**cv))\n\n    return pf\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.get_all_costs","title":"<code>get_all_costs()</code>","text":"<p>All the recorded costs.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def get_all_costs(self) -&gt; dict[str, float]:\n    \"\"\"All the recorded costs.\"\"\"\n    return self._costs\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.get_cost","title":"<code>get_cost(instance)</code>","text":"<p>Get the currently known costs of an instance.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def get_cost(self, instance: str):\n    \"\"\"Get the currently known costs of an instance.\"\"\"\n    return self._costs[instance]\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.get_costs","title":"<code>get_costs(instances)</code>","text":"<p>Get costs of more than one instance.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def get_costs(self, instances: Iterable[str]) -&gt; dict[str, float]:\n    \"\"\"Get costs of more than one instance.\"\"\"\n    costs: dict[str, float] = {}\n\n    for inst in instances:\n        if inst in self._costs:\n            costs[inst] = self._costs[inst]\n\n    return costs\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.get_mean_cost","title":"<code>get_mean_cost()</code>","text":"<p>Get the mean cost.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def get_mean_cost(self) -&gt; float:\n    \"\"\"Get the mean cost.\"\"\"\n    return float(np.mean(list(self._costs.values())))\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.get_median_cost","title":"<code>get_median_cost()</code>","text":"<p>Get the median cost.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def get_median_cost(self) -&gt; float:\n    \"\"\"Get the median cost.\"\"\"\n    return float(np.median(list(self._costs.values())))\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.get_set","title":"<code>get_set()</code>","text":"<p>Get the underlying set.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def get_set(self):\n    \"\"\"Get the underlying set.\"\"\"\n    return self._pf_set\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.get_total_cost","title":"<code>get_total_cost()</code>","text":"<p>Get the total cost.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def get_total_cost(self) -&gt; float:\n    \"\"\"Get the total cost.\"\"\"\n    return float(np.sum(list(self._costs.values())))\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.reallocate_resources","title":"<code>reallocate_resources(strategy=ResourceStrategy.Auto)</code>","text":"<p>Realloacte based on current contents and given strategy.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def reallocate_resources(self, strategy: ResourceStrategy = ResourceStrategy.Auto):\n    \"\"\"Realloacte based on current contents and given strategy.\"\"\"\n    if strategy != ResourceStrategy.Auto:\n        raise NotImplementedError(\"Given `ResourceStrategy` is not supported yet.\")\n\n    # NOTE: Should put this alloc stuff in a function\n    n_cores = cpu_count()\n    cores_per = n_cores // len(self)\n    cores_remainder = n_cores % len(self)\n\n    for cv in self:\n        verifier = cv.verifier\n        resources = cv.resources\n        config = cv.configuration\n\n        extra_core = 0\n        if cores_remainder &gt; 0:\n            extra_core = 1\n            cores_remainder -= 1\n\n        new_resources = (cores_per + extra_core, resources[1]) if resources else None\n\n        self.discard(cv)\n        self.add(ConfiguredVerifier(verifier, config, new_resources))\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.str_compact","title":"<code>str_compact()</code>","text":"<p>Get the portfolio in string form in a compact way.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def str_compact(self) -&gt; str:\n    \"\"\"Get the portfolio in string form in a compact way.\"\"\"\n    cvs: list[str] = []\n\n    for cv in self:\n        cvs.append(\n            \"\\t\".join(\n                [\n                    str(cv.verifier),\n                    str(hash(cv.configuration)),\n                    str(cv.resources),\n                ]\n            )\n        )\n\n    return \"\\n\".join(cvs)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.to_json","title":"<code>to_json(out_json_path)</code>","text":"<p>Write the portfolio in JSON format to the specified path.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def to_json(self, out_json_path: Path):\n    \"\"\"Write the portfolio in JSON format to the specified path.\"\"\"\n    json_list: list[dict[str, Any]] = []\n\n    for cv in self._pf_set:\n        cfg_dict = dict(cv.configuration)\n        to_write = {\n            \"verifier\": cv.verifier,\n            \"configuration\": cfg_dict,\n            \"resources\": cv.resources,\n        }\n        json_list.append(to_write)\n\n    with open(out_json_path, \"w\") as f:\n        json.dump(json_list, f, indent=4, default=str)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.Portfolio.update_costs","title":"<code>update_costs(costs)</code>","text":"<p>Upate the costs based on the new costs mapping.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def update_costs(self, costs: Mapping[str, float]):\n    \"\"\"Upate the costs based on the new costs mapping.\"\"\"\n    for instance, cost in costs.items():\n        if instance not in self._costs:\n            self._costs[instance] = cost\n            continue\n\n        self._costs[instance] = min(self._costs[instance], cost)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.PortfolioScenario","title":"<code>PortfolioScenario</code>  <code>dataclass</code>","text":"<p>Scenario for constructing a parallel portfolio.</p> <p>Attributes:</p> Name Type Description <code>verifiers</code> <code>Sequence[str]</code> <p>The name of the verifiers to consider.</p> <code>resources</code> <code>list[tuple[str, int, int]]</code> <p>How many cores and GPUs the verifiers need.</p> <code>instances</code> <code>Sequence[VerificationInstance]</code> <p>The instances on which the PF is constructed.</p> <code>length</code> <code>int</code> <p>The max length of the PF.</p> <code>seconds_per_iter</code> <code>float</code> <p>Number of seconds for each Hydra iteration.</p> <code>configs_per_iter</code> <code>int</code> <p>Number of configs each iteration.</p> <code>alpha</code> <code>float</code> <p>Tune/Pick time split.</p> <code>added_per_iter</code> <code>int</code> <p>Entries added to the PF per iter.</p> <code>stop_early</code> <code>bool</code> <p>Stop procedure if some early stop conditions are met.</p> <code>resource_strategy</code> <code>ResourceStrategy</code> <p>Strat to divide the resources.</p> <code>output_dir</code> <code>Path | None</code> <p>Dir where logs are stored.</p> <code>vnn_compat_mode</code> <code>bool</code> <p>Use vnn compatability for some verifiers.</p> <code>benchmark</code> <code>str | None</code> <p>VNNCOMP benchmark if vnn_compat_mode is <code>True</code>.</p> <code>verifier_kwargs</code> <code>dict[str, dict[str, Any]] | None</code> <p>Kwargs passed to verifiers.</p> <code>uses_simplified_network</code> <code>Iterable[str] | None</code> <p>If the network uses the dnnv simplified nets.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>@dataclass\nclass PortfolioScenario:\n    \"\"\"Scenario for constructing a parallel portfolio.\n\n    Attributes:\n        verifiers: The name of the verifiers to consider.\n        resources: How many cores and GPUs the verifiers need.\n        instances: The instances on which the PF is constructed.\n        length: The max length of the PF.\n        seconds_per_iter: Number of seconds for each Hydra iteration.\n        configs_per_iter: Number of configs each iteration.\n        alpha: Tune/Pick time split.\n        added_per_iter: Entries added to the PF per iter.\n        stop_early: Stop procedure if some early stop conditions are met.\n        resource_strategy: Strat to divide the resources.\n        output_dir: Dir where logs are stored.\n        vnn_compat_mode: Use vnn compatability for some verifiers.\n        benchmark: VNNCOMP benchmark if vnn_compat_mode is `True`.\n        verifier_kwargs: Kwargs passed to verifiers.\n        uses_simplified_network: If the network uses the dnnv simplified nets.\n    \"\"\"\n\n    verifiers: Sequence[str]\n    resources: list[tuple[str, int, int]]\n    instances: Sequence[VerificationInstance]\n    length: int  # TODO: Rename to max_length?\n    seconds_per_iter: float\n\n    # Optional\n    configs_per_iter: int = 1\n    alpha: float = 0.5  # tune/pick split\n    added_per_iter: int = 1\n    stop_early: bool = True\n    resource_strategy: ResourceStrategy = ResourceStrategy.Auto\n    output_dir: Path | None = None\n    vnn_compat_mode: bool = False\n    benchmark: str | None = None\n    verifier_kwargs: dict[str, dict[str, Any]] | None = None\n    uses_simplified_network: Iterable[str] | None = None\n\n    def __post_init__(self):\n        \"\"\"Validate the PF scenario.\"\"\"\n        if self.added_per_iter &gt; 1 or self.configs_per_iter &gt; 1:\n            raise ValueError(\"Adding more than 1 config per iter not supported yet.\")\n\n        if not 0 &lt;= self.alpha &lt;= 1:\n            raise ValueError(f\"Alpha should be in [0.0, 1.0], got {self.alpha}\")\n\n        if self.output_dir is None:\n            current_time = datetime.datetime.now()\n            formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n            self.output_dir = Path(f\"hydra_out/{formatted_time}\")\n\n        self.tune_budget = self.alpha\n        self.pick_budget = 1 - self.alpha\n\n        if self.added_per_iter &gt; self.length:\n            raise ValueError(\"Entries added per iter should be &lt;= length\")\n\n        if self.vnn_compat_mode and not self.benchmark:\n            raise ValueError(\"Use a benchmark name if vnn_compat_mode=True\")\n\n        if self.vnn_compat_mode and self.verifier_kwargs:\n            raise ValueError(\"Cannot use vnn_compat_mode and verifier_kwargs at the same time.\")\n\n        self.n_iters = math.ceil(self.length / self.added_per_iter)\n        self._verify_resources()\n\n    def _verify_resources(self):\n        # Check for duplicates\n        seen = set()\n        for r in self.resources:\n            if r[0] in seen:\n                raise ValueError(f\"Duplicate name '{r[0]}' in resources\")\n\n            if r[0] not in self.verifiers:\n                raise ValueError(f\"{r[0]} in resources but not in verifiers.\")\n\n            seen.add(r[0])\n\n        for v in self.verifiers:\n            if v not in seen:\n                raise ValueError(f\"Verifier '{v}' in verifiers but not in resources.\")\n\n        if self.resource_strategy == ResourceStrategy.Auto:\n            for r in self.resources:\n                if r[1] != 0:\n                    raise ValueError(\"CPU resources should be 0 when using `Auto`\")\n        else:\n            raise NotImplementedError(f\"ResourceStrategy {self.resource_strategy} is not implemented yet.\")\n\n    def get_smac_scenario_kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Return the SMAC scenario kwargs as a dict.\n\n        Returns:\n            dict[str, Any]: The SMAC scenario as a dict.\n        \"\"\"\n        assert self.output_dir is not None  # This is set in `__post_init__`\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        return {\n            \"instances\": verification_instances_to_smac_instances(self.instances),\n            \"instance_features\": index_features(self.instances),\n            \"output_directory\": self.output_dir,\n        }\n\n    def get_smac_instances(self) -&gt; list[str]:\n        \"\"\"Get the instances of the scenario as SMAC instances.\n\n        Returns:\n            list[str]: The SMAC instances.\n        \"\"\"\n        return verification_instances_to_smac_instances(self.instances)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.PortfolioScenario.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the PF scenario.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the PF scenario.\"\"\"\n    if self.added_per_iter &gt; 1 or self.configs_per_iter &gt; 1:\n        raise ValueError(\"Adding more than 1 config per iter not supported yet.\")\n\n    if not 0 &lt;= self.alpha &lt;= 1:\n        raise ValueError(f\"Alpha should be in [0.0, 1.0], got {self.alpha}\")\n\n    if self.output_dir is None:\n        current_time = datetime.datetime.now()\n        formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.output_dir = Path(f\"hydra_out/{formatted_time}\")\n\n    self.tune_budget = self.alpha\n    self.pick_budget = 1 - self.alpha\n\n    if self.added_per_iter &gt; self.length:\n        raise ValueError(\"Entries added per iter should be &lt;= length\")\n\n    if self.vnn_compat_mode and not self.benchmark:\n        raise ValueError(\"Use a benchmark name if vnn_compat_mode=True\")\n\n    if self.vnn_compat_mode and self.verifier_kwargs:\n        raise ValueError(\"Cannot use vnn_compat_mode and verifier_kwargs at the same time.\")\n\n    self.n_iters = math.ceil(self.length / self.added_per_iter)\n    self._verify_resources()\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.PortfolioScenario.get_smac_instances","title":"<code>get_smac_instances()</code>","text":"<p>Get the instances of the scenario as SMAC instances.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The SMAC instances.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def get_smac_instances(self) -&gt; list[str]:\n    \"\"\"Get the instances of the scenario as SMAC instances.\n\n    Returns:\n        list[str]: The SMAC instances.\n    \"\"\"\n    return verification_instances_to_smac_instances(self.instances)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio.PortfolioScenario.get_smac_scenario_kwargs","title":"<code>get_smac_scenario_kwargs()</code>","text":"<p>Return the SMAC scenario kwargs as a dict.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The SMAC scenario as a dict.</p> Source code in <code>autoverify/portfolio/portfolio.py</code> <pre><code>def get_smac_scenario_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Return the SMAC scenario kwargs as a dict.\n\n    Returns:\n        dict[str, Any]: The SMAC scenario as a dict.\n    \"\"\"\n    assert self.output_dir is not None  # This is set in `__post_init__`\n    self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    return {\n        \"instances\": verification_instances_to_smac_instances(self.instances),\n        \"instance_features\": index_features(self.instances),\n        \"output_directory\": self.output_dir,\n    }\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio_runner.PortfolioRunner","title":"<code>PortfolioRunner</code>","text":"<p>Class to run a portfolio in parallel.</p> Source code in <code>autoverify/portfolio/portfolio_runner.py</code> <pre><code>class PortfolioRunner:\n    \"\"\"Class to run a portfolio in parallel.\"\"\"\n\n    def __init__(\n        self,\n        portfolio: Portfolio,\n        vbs_mode: bool = False,\n        *,\n        n_cpu: int | None = None,\n        n_gpu: int | None = None,\n    ):\n        \"\"\"Initialize a new portfolio runner.\n\n        Arguments:\n            portfolio: The portfolio that will be run.\n            vbs_mode: If the PF will be run in VBS mode.\n            n_cpu: Override number of CPUs\n            n_gpu: Override number of GPUs.\n        \"\"\"\n        self._portfolio = portfolio\n        self._vbs_mode = vbs_mode\n        self._n_cpu = n_cpu\n        self._n_gpu = n_gpu\n\n        if not self._vbs_mode:\n            self._init_resources()\n\n        self._is_cleaning = False\n\n        def _wrap_cleanup(*_):\n            if self._is_cleaning:\n                return\n\n            self._is_cleaning = True\n            self._cleanup()\n            self._is_cleaning = False\n\n        signal.signal(signal.SIGINT, _wrap_cleanup)\n        signal.signal(signal.SIGTERM, _wrap_cleanup)\n\n    def _init_resources(self):\n        self._allocation: dict[ConfiguredVerifier, tuple[int, int, int]] = {}\n        cpu_left = self._n_cpu or cpu_count()\n        gpu_left = self._n_gpu or nvidia_gpu_count()\n\n        for cv in self._portfolio:\n            if cv.resources is None:\n                raise ValueError(f\"No resources for{cv.verifier} :: {cv.configuration} found.\")\n\n            # CPU (taskset) and GPU (CUDA_VISIBLE_DEVICES) index start at 0\n            n_cpu, n_gpu = cv.resources[0], cv.resources[1]\n\n            if n_gpu &gt; gpu_left:\n                raise RuntimeError(\"No more GPUs left\")\n            if n_cpu &gt; cpu_left:\n                raise RuntimeError(\"No more CPUs left\")\n            if n_cpu &lt;= 0:\n                raise RuntimeError(\"CPUs should be &gt; 0\")\n\n            # Currently only support 1 GPU per verifier\n            if n_gpu &gt; 0:\n                curr_gpu = gpu_left - 1\n                gpu_left -= 1\n            else:\n                curr_gpu = -1\n\n            cpu_high = cpu_left\n            cpu_low = cpu_left - n_cpu\n            cpu_left = cpu_low\n\n            self._allocation[cv] = (cpu_low, cpu_high - 1, curr_gpu)\n\n    def evaluate_vbs(\n        self,\n        instances: list[VerificationInstance],\n        *,\n        out_csv: Path | None = None,\n        vnncompat: bool = False,\n        benchmark: str | None = None,\n    ) -&gt; _VbsResult:\n        \"\"\"Evaluate the PF in vbs mode.\n\n        Arguments:\n            instances: Instances to evaluate.\n            out_csv: File where the results are written to.\n            vnncompat: Use some compat kwargs.\n            benchmark: Only if vnncompat, benchmark name.\n        \"\"\"\n        results: _CostDict = {}\n\n        if vnncompat and benchmark is None:\n            raise ValueError(\"Need a benchmark if vnncompat=True\")\n        elif not vnncompat and benchmark is not None:\n            raise ValueError(\"Only use benchmark if vnncompat=True\")\n\n        for cv in self._portfolio:\n            assert cv.resources is not None\n\n            for instance in instances:\n                verifier = _get_verifier(instance, cv, vnncompat, benchmark)\n                logger.info(f\"{cv.verifier} on {str(instance)}\")\n                result = verifier.verify_instance(instance)\n                self._add_result(cv, instance, result, results)\n\n                if out_csv:\n                    self._csv_log_result(\n                        out_csv,\n                        result,\n                        instance,\n                        cv.verifier,\n                        cv.configuration,\n                    )\n\n        return self._vbs_from_cost_dict(results)\n\n    @staticmethod\n    def _csv_log_result(\n        out_csv: Path,\n        result: CompleteVerificationResult,\n        instance: VerificationInstance,\n        verifier: str,\n        configuration: Configuration,\n    ):\n        if isinstance(result, Ok):\n            res_d = result.unwrap()\n            success = \"OK\"\n        elif isinstance(result, Err):\n            res_d = result.unwrap_err()\n            success = \"ERR\"\n\n        inst_d = {\n            \"network\": instance.network,\n            \"property\": instance.property,\n            \"timeout\": instance.timeout,\n            \"verifier\": verifier,\n            \"config\": configuration,\n            \"success\": success,\n        }\n\n        if not out_csv.exists():\n            init_verification_result_csv(out_csv)\n\n        vdr = VerificationDataResult.from_verification_result(res_d, inst_d)\n        csv_append_verification_result(vdr, out_csv)\n\n    @staticmethod\n    def _vbs_from_cost_dict(cost_dict: _CostDict) -&gt; _VbsResult:\n        vbs: _VbsResult = {}\n\n        for cv, instance_costs in cost_dict.items():\n            for instance, cost in instance_costs.items():\n                str_inst = str(instance)\n\n                if str_inst not in vbs:\n                    vbs[str_inst] = (cost, cv.verifier)\n                    continue\n\n                if cost &lt; vbs[str_inst][0]:\n                    vbs[str_inst] = (cost, cv.verifier)\n\n        return vbs\n\n    @staticmethod\n    def _add_result(\n        cv: ConfiguredVerifier,\n        instance: VerificationInstance,\n        result: CompleteVerificationResult,\n        results: _CostDict,\n    ):\n        cost: float\n\n        if isinstance(result, Ok):\n            cost = result.unwrap().took\n        elif isinstance(result, Err):\n            cost = float(\"inf\")\n\n        logger.info(f\"Cost: {cost}\")\n\n        if cv not in results:\n            results[cv] = {}\n\n        results[cv][instance] = cost\n\n    # TODO: Arg types\n    def _get_verifiers(\n        self,\n        instance,\n        vnncompat,\n        benchmark,\n        verifier_kwargs,\n    ) -&gt; dict[ConfiguredVerifier, CompleteVerifier]:\n        verifiers: dict[ConfiguredVerifier, CompleteVerifier] = {}\n\n        for cv in self._portfolio:\n            v = _get_verifier(\n                instance,\n                cv,\n                vnncompat,\n                benchmark,\n                verifier_kwargs,\n                self._allocation,\n            )\n            assert isinstance(v, CompleteVerifier)\n            verifiers[cv] = v\n\n        return verifiers\n\n    def verify_instances(\n        self,\n        instances: Iterable[VerificationInstance],\n        *,\n        out_csv: Path | None = None,\n        vnncompat: bool = False,\n        benchmark: str | None = None,\n        verifier_kwargs: dict[str, dict[str, Any]] | None = None,\n        uses_simplified_network: Iterable[str] | None = None,\n    ) -&gt; dict[VerificationInstance, VerificationDataResult]:\n        \"\"\"Run the PF in parallel.\n\n        Arguments:\n            instances: Instances to evaluate.\n            out_csv: File where the results are written to.\n            vnncompat: Use some compat kwargs.\n            benchmark: Only if vnncompat, benchmark name.\n            verifier_kwargs: Kwargs passed to verifiers.\n            uses_simplified_network: Have some verifiers use simplified nets.\n        \"\"\"\n        if self._vbs_mode:\n            raise RuntimeError(\"Function not compatible with vbs_mode\")\n\n        if out_csv:\n            out_csv = out_csv.expanduser().resolve()\n\n        results: dict[VerificationInstance, VerificationDataResult] = {}\n\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            for instance in instances:\n                logger.info(f\"Running portfolio on {str(instance)}\")\n\n                futures: dict[Future[CompleteVerificationResult], ConfiguredVerifier] = {}\n                self._verifiers = self._get_verifiers(\n                    instance,\n                    vnncompat,\n                    benchmark,\n                    verifier_kwargs,\n                )\n                is_solved = False\n\n                for cv in self._portfolio:\n                    if uses_simplified_network and cv.verifier in uses_simplified_network:\n                        target_instance = instance.as_simplified_network()\n                    else:\n                        target_instance = instance\n\n                    future = executor.submit(self._verifiers[cv].verify_instance, target_instance)\n                    futures[future] = cv\n\n                for future in concurrent.futures.as_completed(futures):\n                    fut_cv = futures[future]\n                    result = future.result()\n\n                    if not is_solved:\n                        got_solved = self._process_result(result, results, fut_cv, instance, self._verifiers)\n\n                        if got_solved and not is_solved:\n                            is_solved = True\n\n                        if out_csv:\n                            self._csv_log_result(\n                                out_csv,\n                                result,\n                                instance,\n                                fut_cv.verifier,\n                                fut_cv.configuration,\n                            )\n\n        return results\n\n    def _process_result(\n        self,\n        result: CompleteVerificationResult,\n        results: dict[VerificationInstance, VerificationDataResult],\n        cv: ConfiguredVerifier,\n        instance: VerificationInstance,\n        verifiers: dict[ConfiguredVerifier, CompleteVerifier],\n    ) -&gt; bool:\n        instance_data: dict[str, Any] = {\n            \"network\": instance.network,\n            \"property\": instance.property,\n            \"timeout\": instance.timeout,\n            \"verifier\": cv.verifier,\n            \"config\": cv.configuration,\n        }\n\n        if isinstance(result, Ok):\n            r = result.unwrap()\n\n            if r.result == \"TIMEOUT\":\n                log_string = f\"{cv.verifier} timed out\"\n            else:\n                self._cancel_running(cv, verifiers)\n                log_string = f\"Verified by {cv.verifier} in {r.took:.2f} sec, result = {r.result}\"\n\n            instance_data[\"success\"] = \"OK\"\n            results[instance] = VerificationDataResult.from_verification_result(r, instance_data)\n\n            # Signal that the instance was solved\n            if r.result in [\"SAT\", \"UNSAT\"]:\n                logger.info(log_string)\n                return True\n        elif isinstance(result, Err):\n            log_string = f\"{cv.verifier} errored.\"\n            r = result.unwrap_err()\n\n            instance_data[\"success\"] = \"ERR\"\n            results[instance] = VerificationDataResult.from_verification_result(r, instance_data)\n\n        logger.info(log_string)\n        # Instance was not solved\n        return False\n\n    def _cancel_running(\n        self,\n        first_cv: ConfiguredVerifier,\n        verifiers: dict[ConfiguredVerifier, CompleteVerifier],\n    ):\n        others = set_iter_except(self._portfolio.get_set(), first_cv)\n        for other_cv in others:\n            verifiers[other_cv].set_timeout_event()\n\n    def _cleanup(self):\n        \"\"\"Kill all running verifiers processes.\"\"\"\n        if not self._verifiers:\n            sys.exit(0)\n\n        for verifier_inst in self._verifiers.values():\n            try:\n                verifier_inst.set_timeout_event()\n            finally:\n                pass\n\n        sys.exit(0)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio_runner.PortfolioRunner.__init__","title":"<code>__init__(portfolio, vbs_mode=False, *, n_cpu=None, n_gpu=None)</code>","text":"<p>Initialize a new portfolio runner.</p> <p>Parameters:</p> Name Type Description Default <code>portfolio</code> <code>Portfolio</code> <p>The portfolio that will be run.</p> required <code>vbs_mode</code> <code>bool</code> <p>If the PF will be run in VBS mode.</p> <code>False</code> <code>n_cpu</code> <code>int | None</code> <p>Override number of CPUs</p> <code>None</code> <code>n_gpu</code> <code>int | None</code> <p>Override number of GPUs.</p> <code>None</code> Source code in <code>autoverify/portfolio/portfolio_runner.py</code> <pre><code>def __init__(\n    self,\n    portfolio: Portfolio,\n    vbs_mode: bool = False,\n    *,\n    n_cpu: int | None = None,\n    n_gpu: int | None = None,\n):\n    \"\"\"Initialize a new portfolio runner.\n\n    Arguments:\n        portfolio: The portfolio that will be run.\n        vbs_mode: If the PF will be run in VBS mode.\n        n_cpu: Override number of CPUs\n        n_gpu: Override number of GPUs.\n    \"\"\"\n    self._portfolio = portfolio\n    self._vbs_mode = vbs_mode\n    self._n_cpu = n_cpu\n    self._n_gpu = n_gpu\n\n    if not self._vbs_mode:\n        self._init_resources()\n\n    self._is_cleaning = False\n\n    def _wrap_cleanup(*_):\n        if self._is_cleaning:\n            return\n\n        self._is_cleaning = True\n        self._cleanup()\n        self._is_cleaning = False\n\n    signal.signal(signal.SIGINT, _wrap_cleanup)\n    signal.signal(signal.SIGTERM, _wrap_cleanup)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio_runner.PortfolioRunner.evaluate_vbs","title":"<code>evaluate_vbs(instances, *, out_csv=None, vnncompat=False, benchmark=None)</code>","text":"<p>Evaluate the PF in vbs mode.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[VerificationInstance]</code> <p>Instances to evaluate.</p> required <code>out_csv</code> <code>Path | None</code> <p>File where the results are written to.</p> <code>None</code> <code>vnncompat</code> <code>bool</code> <p>Use some compat kwargs.</p> <code>False</code> <code>benchmark</code> <code>str | None</code> <p>Only if vnncompat, benchmark name.</p> <code>None</code> Source code in <code>autoverify/portfolio/portfolio_runner.py</code> <pre><code>def evaluate_vbs(\n    self,\n    instances: list[VerificationInstance],\n    *,\n    out_csv: Path | None = None,\n    vnncompat: bool = False,\n    benchmark: str | None = None,\n) -&gt; _VbsResult:\n    \"\"\"Evaluate the PF in vbs mode.\n\n    Arguments:\n        instances: Instances to evaluate.\n        out_csv: File where the results are written to.\n        vnncompat: Use some compat kwargs.\n        benchmark: Only if vnncompat, benchmark name.\n    \"\"\"\n    results: _CostDict = {}\n\n    if vnncompat and benchmark is None:\n        raise ValueError(\"Need a benchmark if vnncompat=True\")\n    elif not vnncompat and benchmark is not None:\n        raise ValueError(\"Only use benchmark if vnncompat=True\")\n\n    for cv in self._portfolio:\n        assert cv.resources is not None\n\n        for instance in instances:\n            verifier = _get_verifier(instance, cv, vnncompat, benchmark)\n            logger.info(f\"{cv.verifier} on {str(instance)}\")\n            result = verifier.verify_instance(instance)\n            self._add_result(cv, instance, result, results)\n\n            if out_csv:\n                self._csv_log_result(\n                    out_csv,\n                    result,\n                    instance,\n                    cv.verifier,\n                    cv.configuration,\n                )\n\n    return self._vbs_from_cost_dict(results)\n</code></pre>"},{"location":"api/#autoverify.portfolio.portfolio_runner.PortfolioRunner.verify_instances","title":"<code>verify_instances(instances, *, out_csv=None, vnncompat=False, benchmark=None, verifier_kwargs=None, uses_simplified_network=None)</code>","text":"<p>Run the PF in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>Iterable[VerificationInstance]</code> <p>Instances to evaluate.</p> required <code>out_csv</code> <code>Path | None</code> <p>File where the results are written to.</p> <code>None</code> <code>vnncompat</code> <code>bool</code> <p>Use some compat kwargs.</p> <code>False</code> <code>benchmark</code> <code>str | None</code> <p>Only if vnncompat, benchmark name.</p> <code>None</code> <code>verifier_kwargs</code> <code>dict[str, dict[str, Any]] | None</code> <p>Kwargs passed to verifiers.</p> <code>None</code> <code>uses_simplified_network</code> <code>Iterable[str] | None</code> <p>Have some verifiers use simplified nets.</p> <code>None</code> Source code in <code>autoverify/portfolio/portfolio_runner.py</code> <pre><code>def verify_instances(\n    self,\n    instances: Iterable[VerificationInstance],\n    *,\n    out_csv: Path | None = None,\n    vnncompat: bool = False,\n    benchmark: str | None = None,\n    verifier_kwargs: dict[str, dict[str, Any]] | None = None,\n    uses_simplified_network: Iterable[str] | None = None,\n) -&gt; dict[VerificationInstance, VerificationDataResult]:\n    \"\"\"Run the PF in parallel.\n\n    Arguments:\n        instances: Instances to evaluate.\n        out_csv: File where the results are written to.\n        vnncompat: Use some compat kwargs.\n        benchmark: Only if vnncompat, benchmark name.\n        verifier_kwargs: Kwargs passed to verifiers.\n        uses_simplified_network: Have some verifiers use simplified nets.\n    \"\"\"\n    if self._vbs_mode:\n        raise RuntimeError(\"Function not compatible with vbs_mode\")\n\n    if out_csv:\n        out_csv = out_csv.expanduser().resolve()\n\n    results: dict[VerificationInstance, VerificationDataResult] = {}\n\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        for instance in instances:\n            logger.info(f\"Running portfolio on {str(instance)}\")\n\n            futures: dict[Future[CompleteVerificationResult], ConfiguredVerifier] = {}\n            self._verifiers = self._get_verifiers(\n                instance,\n                vnncompat,\n                benchmark,\n                verifier_kwargs,\n            )\n            is_solved = False\n\n            for cv in self._portfolio:\n                if uses_simplified_network and cv.verifier in uses_simplified_network:\n                    target_instance = instance.as_simplified_network()\n                else:\n                    target_instance = instance\n\n                future = executor.submit(self._verifiers[cv].verify_instance, target_instance)\n                futures[future] = cv\n\n            for future in concurrent.futures.as_completed(futures):\n                fut_cv = futures[future]\n                result = future.result()\n\n                if not is_solved:\n                    got_solved = self._process_result(result, results, fut_cv, instance, self._verifiers)\n\n                    if got_solved and not is_solved:\n                        is_solved = True\n\n                    if out_csv:\n                        self._csv_log_result(\n                            out_csv,\n                            result,\n                            instance,\n                            fut_cv.verifier,\n                            fut_cv.configuration,\n                        )\n\n    return results\n</code></pre>"},{"location":"api/#util","title":"Util","text":"<p>summary.</p> <p>VerificationInstance.</p> <p>Verifier VNNCOMP compatability.</p> <p>Return verifier instances that should be compatible with the given benchmark + instance.</p>"},{"location":"api/#autoverify.util.instances.VerificationDataResult","title":"<code>VerificationDataResult</code>  <code>dataclass</code>","text":"<p>summary.</p> Source code in <code>autoverify/util/instances.py</code> <pre><code>@dataclass\nclass VerificationDataResult:\n    \"\"\"_summary_.\"\"\"\n\n    network: str\n    property: str\n    timeout: int | None\n    verifier: str\n    config: str\n    success: Literal[\"OK\", \"ERR\"]\n    result: VerificationResultString\n    took: float\n    counter_example: str | tuple[str, str] | None\n    stderr: str | None\n    stdout: str | None\n\n    def __post_init__(self):\n        \"\"\"_summary_.\"\"\"\n        if self.config == \"None\":\n            self.config = \"default\"\n\n    def as_csv_row(self) -&gt; list[str]:\n        \"\"\"Convert data to a csv row writeable.\"\"\"\n        if isinstance(self.counter_example, tuple):\n            self.counter_example = \"\\n\".join(self.counter_example)\n\n        return [\n            self.network,\n            self.property,\n            str(self.timeout),\n            self.verifier,\n            self.config,\n            self.success,\n            self.result,\n            str(self.took),\n            self.counter_example or \"\",\n            self.stderr or \"\",\n            self.stdout or \"\",\n        ]\n\n    @classmethod\n    def from_verification_result(\n        cls,\n        verif_res: CompleteVerificationData,\n        instance_data: dict[str, Any],  # TODO: Narrow Any\n    ):\n        \"\"\"Create from a `CompleteVerificationData`.\"\"\"\n        # TODO: Unpack dict\n        return cls(\n            instance_data[\"network\"],\n            instance_data[\"property\"],\n            instance_data[\"timeout\"],\n            instance_data[\"verifier\"],\n            instance_data[\"config\"],\n            instance_data[\"success\"],\n            verif_res.result,\n            verif_res.took,\n            verif_res.counter_example,\n            verif_res.err,\n            verif_res.stdout,\n        )\n</code></pre>"},{"location":"api/#autoverify.util.instances.VerificationDataResult.__post_init__","title":"<code>__post_init__()</code>","text":"<p>summary.</p> Source code in <code>autoverify/util/instances.py</code> <pre><code>def __post_init__(self):\n    \"\"\"_summary_.\"\"\"\n    if self.config == \"None\":\n        self.config = \"default\"\n</code></pre>"},{"location":"api/#autoverify.util.instances.VerificationDataResult.as_csv_row","title":"<code>as_csv_row()</code>","text":"<p>Convert data to a csv row writeable.</p> Source code in <code>autoverify/util/instances.py</code> <pre><code>def as_csv_row(self) -&gt; list[str]:\n    \"\"\"Convert data to a csv row writeable.\"\"\"\n    if isinstance(self.counter_example, tuple):\n        self.counter_example = \"\\n\".join(self.counter_example)\n\n    return [\n        self.network,\n        self.property,\n        str(self.timeout),\n        self.verifier,\n        self.config,\n        self.success,\n        self.result,\n        str(self.took),\n        self.counter_example or \"\",\n        self.stderr or \"\",\n        self.stdout or \"\",\n    ]\n</code></pre>"},{"location":"api/#autoverify.util.instances.VerificationDataResult.from_verification_result","title":"<code>from_verification_result(verif_res, instance_data)</code>  <code>classmethod</code>","text":"<p>Create from a <code>CompleteVerificationData</code>.</p> Source code in <code>autoverify/util/instances.py</code> <pre><code>@classmethod\ndef from_verification_result(\n    cls,\n    verif_res: CompleteVerificationData,\n    instance_data: dict[str, Any],  # TODO: Narrow Any\n):\n    \"\"\"Create from a `CompleteVerificationData`.\"\"\"\n    # TODO: Unpack dict\n    return cls(\n        instance_data[\"network\"],\n        instance_data[\"property\"],\n        instance_data[\"timeout\"],\n        instance_data[\"verifier\"],\n        instance_data[\"config\"],\n        instance_data[\"success\"],\n        verif_res.result,\n        verif_res.took,\n        verif_res.counter_example,\n        verif_res.err,\n        verif_res.stdout,\n    )\n</code></pre>"},{"location":"api/#autoverify.util.instances.csv_append_verification_result","title":"<code>csv_append_verification_result(verification_result, csv_path)</code>","text":"<p>summary.</p> Source code in <code>autoverify/util/instances.py</code> <pre><code>def csv_append_verification_result(verification_result: VerificationDataResult, csv_path: Path):\n    \"\"\"_summary_.\"\"\"\n    with open(str(csv_path.expanduser()), \"a\") as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(verification_result.as_csv_row())\n</code></pre>"},{"location":"api/#autoverify.util.instances.init_verification_result_csv","title":"<code>init_verification_result_csv(csv_path)</code>","text":"<p>summary.</p> Source code in <code>autoverify/util/instances.py</code> <pre><code>def init_verification_result_csv(csv_path: Path):\n    \"\"\"_summary_.\"\"\"\n    with open(str(csv_path.expanduser()), \"w\") as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(get_dataclass_field_names(VerificationDataResult))\n</code></pre>"},{"location":"api/#autoverify.util.instances.read_all_vnncomp_instances","title":"<code>read_all_vnncomp_instances(vnncomp_path)</code>","text":"<p>Reads all benchmarks, see the <code>read_vnncomp_instances</code> docstring.</p> Source code in <code>autoverify/util/instances.py</code> <pre><code>def read_all_vnncomp_instances(\n    vnncomp_path: Path,\n) -&gt; dict[str, list[VerificationInstance]]:\n    \"\"\"Reads all benchmarks, see the `read_vnncomp_instances` docstring.\"\"\"\n    instances: dict[str, list[VerificationInstance]] = {}\n\n    for path in vnncomp_path.iterdir():\n        if not path.is_dir():\n            continue\n\n        instances[path.name] = read_vnncomp_instances(path.name, vnncomp_path)\n\n    return instances\n</code></pre>"},{"location":"api/#autoverify.util.instances.read_verification_result_from_csv","title":"<code>read_verification_result_from_csv(csv_path)</code>","text":"<p>Reads a verification results csv to a list of its dataclass.</p> Source code in <code>autoverify/util/instances.py</code> <pre><code>def read_verification_result_from_csv(\n    csv_path: Path,\n) -&gt; list[VerificationDataResult]:\n    \"\"\"Reads a verification results csv to a list of its dataclass.\"\"\"\n    results_df = pd.read_csv(csv_path)\n    verification_results: list[VerificationDataResult] = []\n\n    for _, row in results_df.iterrows():\n        row = row.to_list()\n        verification_results.append(VerificationDataResult(*row))\n\n    return verification_results\n</code></pre>"},{"location":"api/#autoverify.util.instances.read_vnncomp_instances","title":"<code>read_vnncomp_instances(benchmark, vnncomp_path, *, predicate=None, as_smac=False, resolve_paths=True, instance_file_name='instances.csv')</code>","text":"<pre><code>read_vnncomp_instances(benchmark: str, vnncomp_path: Path, *, predicate: _InstancePredicate | Iterable[_InstancePredicate] | None = None, as_smac: Literal[False] = False, resolve_paths: bool = True, instance_file_name: str = 'instances.csv') -&gt; list[VerificationInstance]\n</code></pre><pre><code>read_vnncomp_instances(benchmark: str, vnncomp_path: Path, *, predicate: _InstancePredicate | Iterable[_InstancePredicate] | None = None, as_smac: Literal[True] = True, resolve_paths: bool = True, instance_file_name: str = 'instances.csv') -&gt; list[str]\n</code></pre> <p>Read the instances of a VNNCOMP benchmark.</p> <p>Reads the CSV file of a VNNCOMP benchmark, parsing the network, property and timeout values.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark</code> <code>str</code> <p>The name of the benchmark directory.</p> required <code>vnncomp_path</code> <code>Path</code> <p>The path to the VNNCOMP benchmark directory</p> required <code>predicate</code> <code>_InstancePredicate | Iterable[_InstancePredicate] | None</code> <p>A function that, given a <code>VerificationInstance</code> returns either True or False. If False is returned, the <code>VerificationInstance</code> is dropped from the returned instances.</p> <code>None</code> <code>as_smac</code> <code>bool</code> <p>Return the instances as smac instance strings.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[VerificationInstance] | list[str]</code> <p>list[VerificationInstance] | list[str]: A list of <code>VerificationInstance</code> or string objects that hold the network, property and timeout.</p> Source code in <code>autoverify/util/instances.py</code> <pre><code>def read_vnncomp_instances(\n    benchmark: str,\n    vnncomp_path: Path,\n    *,\n    predicate: _InstancePredicate | Iterable[_InstancePredicate] | None = None,\n    as_smac: bool = False,\n    resolve_paths: bool = True,\n    instance_file_name: str = \"instances.csv\",\n) -&gt; list[VerificationInstance] | list[str]:\n    \"\"\"Read the instances of a VNNCOMP benchmark.\n\n    Reads the CSV file of a VNNCOMP benchmark, parsing the network, property and\n    timeout values.\n\n    Args:\n        benchmark: The name of the benchmark directory.\n        vnncomp_path: The path to the VNNCOMP benchmark directory\n        predicate: A function that, given a `VerificationInstance` returns\n            either True or False. If False is returned, the\n            `VerificationInstance` is dropped from the returned instances.\n        as_smac: Return the instances as smac instance strings.\n\n    Returns:\n        list[VerificationInstance] | list[str]: A list of\n            `VerificationInstance` or string objects\n            that hold the network, property and timeout.\n    \"\"\"\n    if not vnncomp_path.is_dir():\n        raise ValueError(\"Could not find VNNCOMP directory\")\n\n    benchmark_dir = vnncomp_path / benchmark\n\n    if not benchmark_dir.is_dir():\n        raise ValueError(f\"{benchmark} is not a valid benchmark in {str(vnncomp_path)}\")\n\n    instances = benchmark_dir / instance_file_name\n    verification_instances = []\n\n    if predicate and not isinstance(predicate, Iterable):\n        predicate = [predicate]\n\n    with open(str(instances)) as csv_file:\n        reader = csv.reader(csv_file)\n\n        for row in reader:\n            network, property, timeout = row\n\n            net_path = Path(str(benchmark_dir / network))\n            prop_path = Path(str(benchmark_dir / property))\n\n            instance = VerificationInstance(\n                net_path if not resolve_paths else net_path.resolve(),\n                prop_path if not resolve_paths else prop_path.resolve(),\n                int(float(timeout)),  # FIXME: Timeouts can be floats\n            )\n\n            if predicate and not _passes_at_least_1(predicate, instance):\n                continue\n\n            if as_smac:\n                instance = instance.as_smac_instance()  # type: ignore\n\n            verification_instances.append(instance)\n\n    return verification_instances\n</code></pre>"},{"location":"api/#autoverify.util.instances.unique_networks","title":"<code>unique_networks(instances)</code>","text":"<p>Utility function to keep only unique networks.</p> Source code in <code>autoverify/util/instances.py</code> <pre><code>def unique_networks(\n    instances: Iterable[VerificationInstance],\n) -&gt; list[VerificationInstance]:\n    \"\"\"Utility function to keep only unique networks.\"\"\"\n    unique = []\n    seen: set[str] = set()\n\n    for instance in instances:\n        if instance.network.name not in seen:\n            unique.append(instance)\n        seen.add(instance.network.name)\n\n    return unique\n</code></pre>"},{"location":"api/#autoverify.util.instances.verification_instances_to_smac_instances","title":"<code>verification_instances_to_smac_instances(instances)</code>","text":"<p>Convert a list of <code>VerificationInstace</code> objects to SMAC instances.</p> <p>See the <code>as_smac_instance</code> docstring of the <code>VerificationInstance</code> class for more details.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>Sequence[VerificationInstance]</code> <p>The list of <code>VerificationInstance</code> objects to convert.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The SMAC instance strings.</p> Source code in <code>autoverify/util/instances.py</code> <pre><code>def verification_instances_to_smac_instances(\n    instances: Sequence[VerificationInstance],\n) -&gt; list[str]:\n    \"\"\"Convert a list of `VerificationInstace` objects to SMAC instances.\n\n    See the `as_smac_instance` docstring of the `VerificationInstance` class for\n    more details.\n\n    Args:\n        instances: The list of `VerificationInstance` objects to convert.\n\n    Returns:\n        list[str]: The SMAC instance strings.\n    \"\"\"\n    return [inst.as_smac_instance() for inst in instances]\n</code></pre>"},{"location":"api/#autoverify.util.instances.write_verification_results_to_csv","title":"<code>write_verification_results_to_csv(results, csv_path)</code>","text":"<p>Writes a verification results df to a csv.</p> Source code in <code>autoverify/util/instances.py</code> <pre><code>def write_verification_results_to_csv(results: pd.DataFrame, csv_path: Path):\n    \"\"\"Writes a verification results df to a csv.\"\"\"\n    results.to_csv(csv_path, index=False)\n</code></pre>"},{"location":"api/#autoverify.util.verification_instance.VerificationInstance","title":"<code>VerificationInstance</code>  <code>dataclass</code>","text":"<p>VerificationInstance class.</p> <p>Attributes:</p> Name Type Description <code>network</code> <code>Path</code> <p>The <code>Path</code> to the network.</p> <code>property</code> <code>Path</code> <p>The <code>Path</code> to the property.</p> <code>timeout</code> <code>int</code> <p>Maximum wallclock time.</p> Source code in <code>autoverify/util/verification_instance.py</code> <pre><code>@dataclass(frozen=True, eq=True)\nclass VerificationInstance:\n    \"\"\"VerificationInstance class.\n\n    Attributes:\n        network: The `Path` to the network.\n        property: The `Path` to the property.\n        timeout: Maximum wallclock time.\n    \"\"\"\n\n    network: Path\n    property: Path\n    timeout: int\n\n    @classmethod\n    def from_str(cls, str_instance: str):\n        \"\"\"Create from a comma seperated string.\"\"\"\n        network, property, timeout = str_instance.split(\",\")\n        return cls(Path(network), Path(property), int(timeout))\n\n    def __hash__(self):\n        \"\"\"Hash the VI.\"\"\"\n        return hash(\n            (\n                self.network.expanduser().resolve(),\n                self.property.expanduser().resolve(),\n                self.timeout,\n            )\n        )\n\n    def __str__(self):\n        \"\"\"Short string representation of the `VerificationInstance`.\"\"\"\n        return f\"{self.network.name} :: {self.property.name} :: {self.timeout}\"\n\n    def as_smac_instance(self) -&gt; str:\n        \"\"\"Return the instance in a `f\"{network},{property},{timeout}\"` format.\n\n        A SMAC instance has to be passed as a single string to the\n        target_function, in which we split the instance string on the comma\n        again to obtain the network, property and timeout.\n\n        If no timeout is specified, the `DEFAULT_VERIFICATION_TIMEOUT_SEC`\n        global is used.\n\n        Returns:\n            str: The smac instance string\n        \"\"\"\n        timeout: int = self.timeout or DEFAULT_VERIFICATION_TIMEOUT_SEC\n\n        return f\"{str(self.network)},{str(self.property)},{str(timeout)}\"\n\n    def as_row(self, resolve_paths: bool = True) -&gt; list[str]:\n        \"\"\"Returns the instance as a list of strings.\"\"\"\n        net = str(self.network.expanduser().resolve()) if resolve_paths else str(self.network)\n\n        prop = str(self.property.expanduser().resolve()) if resolve_paths else str(self.property)\n\n        return [net, prop, str(self.timeout)]\n\n    # HACK: Assuming some names and layouts here...\n    def as_simplified_network(self) -&gt; VerificationInstance:\n        \"\"\"Changes the network path.\n\n        Assumes a \"onnx_simplified\" dir is present at the same level.\n        \"\"\"\n        simplified_nets_dir = self.network.parent.parent / \"onnx_simplified\"\n\n        return VerificationInstance(simplified_nets_dir / self.network.name, self.property, self.timeout)\n</code></pre>"},{"location":"api/#autoverify.util.verification_instance.VerificationInstance.__hash__","title":"<code>__hash__()</code>","text":"<p>Hash the VI.</p> Source code in <code>autoverify/util/verification_instance.py</code> <pre><code>def __hash__(self):\n    \"\"\"Hash the VI.\"\"\"\n    return hash(\n        (\n            self.network.expanduser().resolve(),\n            self.property.expanduser().resolve(),\n            self.timeout,\n        )\n    )\n</code></pre>"},{"location":"api/#autoverify.util.verification_instance.VerificationInstance.__str__","title":"<code>__str__()</code>","text":"<p>Short string representation of the <code>VerificationInstance</code>.</p> Source code in <code>autoverify/util/verification_instance.py</code> <pre><code>def __str__(self):\n    \"\"\"Short string representation of the `VerificationInstance`.\"\"\"\n    return f\"{self.network.name} :: {self.property.name} :: {self.timeout}\"\n</code></pre>"},{"location":"api/#autoverify.util.verification_instance.VerificationInstance.as_row","title":"<code>as_row(resolve_paths=True)</code>","text":"<p>Returns the instance as a list of strings.</p> Source code in <code>autoverify/util/verification_instance.py</code> <pre><code>def as_row(self, resolve_paths: bool = True) -&gt; list[str]:\n    \"\"\"Returns the instance as a list of strings.\"\"\"\n    net = str(self.network.expanduser().resolve()) if resolve_paths else str(self.network)\n\n    prop = str(self.property.expanduser().resolve()) if resolve_paths else str(self.property)\n\n    return [net, prop, str(self.timeout)]\n</code></pre>"},{"location":"api/#autoverify.util.verification_instance.VerificationInstance.as_simplified_network","title":"<code>as_simplified_network()</code>","text":"<p>Changes the network path.</p> <p>Assumes a \"onnx_simplified\" dir is present at the same level.</p> Source code in <code>autoverify/util/verification_instance.py</code> <pre><code>def as_simplified_network(self) -&gt; VerificationInstance:\n    \"\"\"Changes the network path.\n\n    Assumes a \"onnx_simplified\" dir is present at the same level.\n    \"\"\"\n    simplified_nets_dir = self.network.parent.parent / \"onnx_simplified\"\n\n    return VerificationInstance(simplified_nets_dir / self.network.name, self.property, self.timeout)\n</code></pre>"},{"location":"api/#autoverify.util.verification_instance.VerificationInstance.as_smac_instance","title":"<code>as_smac_instance()</code>","text":"<p>Return the instance in a <code>f\"{network},{property},{timeout}\"</code> format.</p> <p>A SMAC instance has to be passed as a single string to the target_function, in which we split the instance string on the comma again to obtain the network, property and timeout.</p> <p>If no timeout is specified, the <code>DEFAULT_VERIFICATION_TIMEOUT_SEC</code> global is used.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The smac instance string</p> Source code in <code>autoverify/util/verification_instance.py</code> <pre><code>def as_smac_instance(self) -&gt; str:\n    \"\"\"Return the instance in a `f\"{network},{property},{timeout}\"` format.\n\n    A SMAC instance has to be passed as a single string to the\n    target_function, in which we split the instance string on the comma\n    again to obtain the network, property and timeout.\n\n    If no timeout is specified, the `DEFAULT_VERIFICATION_TIMEOUT_SEC`\n    global is used.\n\n    Returns:\n        str: The smac instance string\n    \"\"\"\n    timeout: int = self.timeout or DEFAULT_VERIFICATION_TIMEOUT_SEC\n\n    return f\"{str(self.network)},{str(self.property)},{str(timeout)}\"\n</code></pre>"},{"location":"api/#autoverify.util.verification_instance.VerificationInstance.from_str","title":"<code>from_str(str_instance)</code>  <code>classmethod</code>","text":"<p>Create from a comma seperated string.</p> Source code in <code>autoverify/util/verification_instance.py</code> <pre><code>@classmethod\ndef from_str(cls, str_instance: str):\n    \"\"\"Create from a comma seperated string.\"\"\"\n    network, property, timeout = str_instance.split(\",\")\n    return cls(Path(network), Path(property), int(timeout))\n</code></pre>"},{"location":"api/#autoverify.util.vnncomp.inst_bench_to_kwargs","title":"<code>inst_bench_to_kwargs(benchmark, verifier, instance)</code>","text":"<p>Get the kwargs for a benchmark.</p> Source code in <code>autoverify/util/vnncomp.py</code> <pre><code>def inst_bench_to_kwargs(\n    benchmark: str,\n    verifier: str,\n    instance: VerificationInstance,\n) -&gt; dict[str, Any]:\n    \"\"\"Get the kwargs for a benchmark.\"\"\"\n    if verifier == \"nnenum\":\n        return {\"use_auto_settings\": True}\n    elif verifier == \"abcrown\":  # TODO: All benchmarks\n        if benchmark == \"acasxu\":\n            return {\"yaml_override\": {\"data__num_outputs\": 5}}\n        elif benchmark.startswith(\"sri_resnet_\"):\n            return {\n                \"yaml_override\": {\n                    \"model__onnx_quirks\": \"{'Reshape': {'fix_batch_size': True}}\"  # noqa: E501\n                }\n            }\n        return {}\n    elif verifier == \"ovalbab\":\n        return {}\n    elif verifier == \"verinet\":\n        if benchmark == \"acasxu\":\n            return {\"transpose_matmul_weights\": True}\n        elif benchmark == \"cifar2020\":\n            if instance.network.name.find(\"convBigRELU\") &gt;= 0:\n                return {\"dnnv_simplify\": True}\n        elif benchmark == \"cifar100_tinyimagenet_resnet\" or (\n            benchmark == \"nn4sys\" and instance.network.name == \"lindex.onnx\"\n        ):\n            return {\"dnnv_simplify\": True}\n        return {}\n\n    raise ValueError(\"Invalid verifier\")\n</code></pre>"},{"location":"api/#autoverify.util.vnncomp.inst_bench_to_verifier","title":"<code>inst_bench_to_verifier(benchmark, instance, verifier, allocation=None)</code>","text":"<p>Get an instantiated verifier.</p> Source code in <code>autoverify/util/vnncomp.py</code> <pre><code>def inst_bench_to_verifier(\n    benchmark: str,\n    instance: VerificationInstance,\n    verifier: str,\n    allocation: tuple[int, int, int] | None = None,\n) -&gt; CompleteVerifier:\n    \"\"\"Get an instantiated verifier.\"\"\"\n    verifier_inst = verifier_from_name(verifier)(\n        **inst_bench_to_kwargs(benchmark, verifier, instance),\n        cpu_gpu_allocation=allocation,\n    )\n    assert isinstance(verifier_inst, CompleteVerifier)\n    return verifier_inst\n</code></pre>"},{"location":"api/#autoverify.util.vnncomp.inst_bench_verifier_config","title":"<code>inst_bench_verifier_config(benchmark, instance, verifier, configs_dir)</code>","text":"<p>Return the verifier and the VNNCOMP config.</p> Source code in <code>autoverify/util/vnncomp.py</code> <pre><code>def inst_bench_verifier_config(\n    benchmark: str,\n    instance: VerificationInstance,\n    verifier: str,\n    configs_dir: Path,\n) -&gt; Configuration | Path | None:\n    \"\"\"Return the verifier and the VNNCOMP config.\"\"\"\n    if benchmark == \"test_props\":\n        return None\n\n    if verifier == \"nnenum\":\n        return None\n    elif verifier == \"abcrown\":\n        cfg_file = _get_abcrown_config(benchmark, instance)\n        return Path(configs_dir / \"abcrown\" / cfg_file)\n    elif verifier == \"ovalbab\":\n        cfg = _get_ovalbab_config(benchmark)\n        if cfg is not None:\n            return Path(configs_dir / \"ovalbab\" / cfg)\n        return cfg\n    elif verifier == \"verinet\":\n        return _get_verinet_config(benchmark)\n\n    raise ValueError(\"Invalid verifier\")\n</code></pre>"},{"location":"how-to-guides/","title":"How-To-Guides","text":""},{"location":"how-to-guides/#verifying-properties","title":"Verifying Properties","text":""},{"location":"how-to-guides/#simple-example","title":"Simple Example","text":"<p>After installing one or more verifiers, here is how to use them to verify a property. Networks should be in the ONNX format, properties in the VNNLIB format.</p> <pre><code>from pathlib import Path\nfrom result import Err, Ok\nfrom autoverify.verifier import AbCrown\n\nif __name__ == \"__main__\":\n    verifier = AbCrown()\n\n    network = Path(\"my_network.onnx\")\n    prop = Path(\"my_property.vnnlib\")\n\n    result = verifier.verify_property(network, prop)\n\n    if isinstance(result, Ok):\n        outcome = result.unwrap().result\n        print(\"Verification finished, result:\", outcome)\n    elif isinstance(result, Err):\n        print(\"Error during verification:\")\n        print(result.unwrap_err().stdout)\n</code></pre>"},{"location":"how-to-guides/#running-vnncomp-benchmarks","title":"Running VNNCOMP Benchmarks","text":"<p>Auto-Verify supports reading benchmarks defined in VNNCOMP style, which are benchmarks with the following structure:</p> <pre><code>vnncomp2022\n\u2514\u2500\u2500 test_props\n    \u251c\u2500\u2500 instances.csv\n    \u251c\u2500\u2500 onnx\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 test_nano.onnx\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 test_sat.onnx\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 test_unsat.onnx\n    \u2514\u2500\u2500 vnnlib\n        \u251c\u2500\u2500 test_nano.vnnlib\n        \u2514\u2500\u2500 test_prop.vnnlib\n</code></pre> <p>Where <code>instances.csv</code> is a <code>csv</code> file with 3 columns: network, property, timeout. For example, the <code>test_props</code> directory contains the following 3 verification instaces:</p> <pre><code>onnx/test_sat.onnx,vnnlib/test_prop.vnnlib,60\nonnx/test_unsat.onnx,vnnlib/test_prop.vnnlib,60\nonnx/test_nano.onnx,vnnlib/test_nano.vnnlib,60\n</code></pre> <p>VNNCOMP Benchmarks can found at the following links: 2022, 2023. Make sure to unzip all files inside the benchmark after you have downloaded it.</p> <p>Below is a code snippet that runs this benchmark. Note the <code>Path</code> pointing to the benchmark location.</p> <pre><code>from pathlib import Path\n\nfrom result import Err, Ok\n\nfrom autoverify.util.instances import read_vnncomp_instances\nfrom autoverify.verifier import Nnenum\n\ntest_props = read_vnncomp_instances(\n    \"test_props\", vnncomp_path=Path(\"../benchmark/vnncomp2022\")\n)\n\nif __name__ == \"__main__\":\n    verifier = Nnenum()\n\n    for instance in test_props:\n        print(instance)\n        result = verifier.verify_instance(instance)\n\n        if isinstance(result, Ok):\n            outcome = result.unwrap().result\n            print(\"Verification finished, result:\", outcome)\n        elif isinstance(result, Err):\n            print(\"Error during verification:\")\n            print(result.unwrap_err().stdout)\n</code></pre>"},{"location":"how-to-guides/#algorithm-configuration","title":"Algorithm Configuration","text":"<p>Each of the verification tools comes equipped with a <code>ConfigurationSpace</code>, which can be used to sample Configuration for a verification tool. For example:</p> <pre><code>from autoverify.verifier import Nnenum\n\nif __name__ == \"__main__\":\n    verifier = Nnenum()\n    config = verifier.config_space.sample_configuration()\n    print(config)\n</code></pre>"},{"location":"how-to-guides/#smac-example","title":"SMAC Example","text":"<p>We can apply algorithm configuration techniques (or hyperparameter optimization) using SMAC. In the example below, we try to find a configuration for AB-CROWN on the first 10 instances of the <code>mnist_fc</code> benchmark from VNNCOMP2022.</p> <pre><code>import sys\nfrom pathlib import Path\n\nfrom ConfigSpace import Configuration\nfrom result import Err, Ok\nfrom smac import AlgorithmConfigurationFacade, Scenario\n\nfrom autoverify.util.instances import (\n    read_vnncomp_instances,\n    verification_instances_to_smac_instances,\n)\nfrom autoverify.util.smac import index_features\nfrom autoverify.util.verification_instance import VerificationInstance\nfrom autoverify.verifier import AbCrown\n\nmnist = read_vnncomp_instances(\"mnist_fc\", vnncomp_path=Path(\"../benchmark/vnncomp2022\"))[:10]\nverifier = AbCrown()\n\ndef target_function(config: Configuration, instance: str, seed: int):\n    seed += 1  # Mute unused var warnings; (cant rename to _)\n    verif_inst = VerificationInstance.from_str(instance)\n    result = verifier.verify_instance(verif_inst, config=config)\n\n    # SMAC handles exception by setting cost to `inf`\n    verification_result = result.unwrap_or_raise(Exception)\n    return float(verification_result.took)\n\nif __name__ == \"__main__\":\n    cfg_space = verifier.config_space\n    name = verifier.name\n    instances = verification_instances_to_smac_instances(mnist)\n\n    scenario = Scenario(\n        cfg_space,\n        name=\"ab_tune\",\n        instances=instances,\n        instance_features=index_features(instances),\n        walltime_limit=600,\n        output_directory=Path(\"ab_tune_out\"),\n        n_trials=sys.maxsize,  # Using walltime limit instead\n    )\n\n    smac = AlgorithmConfigurationFacade(\n        scenario, target_function, overwrite=True\n    )\n\n    inc = smac.optimize()\n    print(inc)\n</code></pre> <p>For more examples on how to use SMAC, please refer to the SMAC documentation.</p>"},{"location":"how-to-guides/#parallel-portfolios","title":"Parallel Portfolios","text":"<p>Note</p> <p>Custom verification tools are not yet supported for parallel portfolios.</p>"},{"location":"how-to-guides/#constructiong-a-portfolio","title":"Constructiong a Portfolio","text":"<p>Portfolios can be constructed as shown below. In this example, we try construct a portfolio using the <code>Hydra</code> algorithm on the <code>mnist_fc</code> benchmark from VNNCOMP2022. We include four verification tools that are able to be included and give the procedure a walltime limit of 24 hours.</p> <pre><code>from pathlib import Path\n\nfrom autoverify.portfolio import Hydra, PortfolioScenario\nfrom autoverify.util.instances import read_vnncomp_instances\n\nbenchmark = read_vnncomp_instances(\"mnist_fc\", vnncomp_path=Path(\"../benchmark/vnncomp2022\"))\n\nif __name__ == \"__main__\":\n    pf_scenario = PortfolioScenario(\n        [\"nnenum\", \"abcrown\", \"ovalbab\", \"verinet\"],\n        [\n            (\"nnenum\", 0, 0),\n            (\"verinet\", 0, 1),\n            (\"abcrown\", 0, 1),\n            (\"ovalbab\", 0, 1),\n        ],\n        benchmark,\n        4,\n        (60 * 60 * 24) / 4,\n        alpha=0.9,\n        output_dir=Path(\"PF_mnist_fc\"),\n    )\n\n    hydra = Hydra(pf_scenario)\n    pf = hydra.tune_portfolio()\n    pf.to_json(Path(\"mnist_fc_portfolio.json\"))\n</code></pre> <p>Portfolios can be manually created as shown below. This example creates a portfolio of 2 verifiers (nnenum and AB-Crown), where nnenum is given 4 CPU cores and 0 GPUs and AB-Crown is given 4 cores and 1 GPU.</p> <pre><code>from autoverify.portfolio.portfolio import ConfiguredVerifier, Portfolio\nfrom autoverify.util.verifiers import get_verifier_configspace\n\nif __name__ == \"__main__\":\n    pf = Portfolio(\n        ConfiguredVerifier(\n            \"nnenum\",\n            get_verifier_configspace(\"nnenum\").sample_configuration(),\n            (4, 0),\n        ),\n        ConfiguredVerifier(\n            \"abcrown\",\n            get_verifier_configspace(\"abcrown\").get_default_configuration(),\n            (4, 1),\n        ),\n    )\n\n    print(pf)\n</code></pre>"},{"location":"how-to-guides/#running-a-portfolio","title":"Running a Portfolio","text":"<p>Portfolios can be read from a <code>json</code> or by specifying the verification tools in Python code. Below is an example of how to run a portfolio in parallel on some instances. Lets take the portfolio we created in the previous example and run it on the same benchmark.</p> <pre><code>from pathlib import Path\n\nfrom autoverify.portfolio import Portfolio, PortfolioRunner\nfrom autoverify.util.instances import read_vnncomp_instances\n\n\nbenchmark = read_vnncomp_instances(\"mnist_fc\", vnncomp_path=Path(\"../benchmark/vnncomp2022\"))\n\nif __name__ == \"__main__\":\n    mnist_pf = Portfolio.from_json(Path(\"mnist_fc_portfolio.json\"))\n    pf_runner = PortfolioRunner(mnist_pf)\n\n    pf_runner.verify_instances(\n        benchmark,\n        out_csv=Path(\"PF_mnist_fc_results.csv\"),\n    )\n</code></pre>"}]}